
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Building a basic GPU accelerated application with HIP in Fortran</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/codelab-elements/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab codelab-gaid="TO DO"
                  id="build-a-gpu-app-hip-fortran"
                  title="Building a basic GPU accelerated application with HIP in Fortran"
                  environment="web"
                  feedback-link="https://octoskelo.atlassian.net/servicedesk/customer/portal/1/create/140">
    
      <google-codelab-step label="Introduction" duration="0">
        <p><strong>Last Updated:</strong> 2022-03-17</p>
<h2 is-upgraded><strong>What you will build</strong></h2>
<p>In this codelab, you will port a small Fortran application to GPU hardware using HIP and HIPFort. You will transition a serial CPU-only mini-application to a portable GPU accelerated application, using HIP and HIPFort. </p>
<p>The goal of this codelab is to introduce you to using the HIPFort API and a development practice that can be applied to porting other Fortran applications to GPU hardware with HIPFort.</p>
<h2 is-upgraded><strong>What you will learn</strong></h2>
<ul>
<li>How to develop a GPU porting strategy using application profiles and call graphs.</li>
<li>How to manage GPU memory with HIPFort</li>
<li>How to launch GPU accelerated kernels with HIPFort and ISO C Binding.</li>
<li>How to build GPU accelerated Fortran applications for AMD and Nvidia platforms.</li>
<li>How to verify GPU memory allocation and kernel execution with the rocprof profiler.</li>
</ul>
<h2 is-upgraded><strong>What you will need</strong></h2>
<ul>
<li>A compute platform with AMD or Nvidia GPU(s)</li>
<li>CUDA Toolkit 10 or greater (Nvidia platforms only)</li>
<li>ROCm (v4.2 or greater)</li>
<li>Fortran compiler</li>
</ul>
<aside class="special"><p><strong>Note :</strong> If you are attending a Lunch &amp; Learn or other training activity with AMD &amp; Fluid Numerics, you are encouraged to use the AMD Accelerator Cloud to run on AMD MI100 GPUs and the OCEAN Cluster to run on Nvidia V100 GPUs. </p>
<p>Keep in mind that you can work through this codelab on one system and copy your modified code to the other to show that the code is portable.</p>
</aside>


      </google-codelab-step>
    
      <google-codelab-step label="Start an Interactive Session on the AMD Accelerator Cloud" duration="5">
        <p>If you plan on using the AMD Accelerator Cloud for a portion of this tutorial, you will need to start an interactive session on a compute node to complete this codelab.</p>
<p>The AMD Accelerator Cloud comes with the Slurm job scheduler and a set of compute partitions that provide you access to a variety compute platforms. This section will walk you through how to log in to the AMD Accelerator Cloud and start an interactive session on a compute node with a MI100 GPU. </p>
<aside class="warning"><p><strong>NOTE : </strong>Prior to completing these steps, make sure that you have completed the onboarding process for the AMD Accelerator Cloud. Instructions to login to the cluster should have been provided to you during onboarding.</p>
</aside>
<ol type="1" start="1">
<li>Once you are connected to the AMD Accelerator Cloud login node, you can use srun to start an interactive session on a node with MI100 GPUs. We recommend that you set a time limit for interactive jobs so that you don&#39;t accidentally leave a reserved node idle.</li>
</ol>
<pre>srun -n1 --partition=MI100 --time=1:00:00 --pty /bin/bash</pre>
<ol type="1" start="2">
<li>Once your interactive session starts, you can use the rocm-smi command to verify that GPUs are available</li>
</ol>
<pre>rocm-smi


======================= ROCm System Management Interface =======================
================================= Concise Info =================================
GPU  Temp   AvgPwr  SCLK    MCLK     Fan   Perf  PwrCap  VRAM%  GPU%  
0    33.0c  35.0W   300Mhz  1200Mhz  255%  auto  290.0W    0%   0%    
1    33.0c  39.0W   300Mhz  1200Mhz  255%  auto  290.0W    0%   0%    
2    34.0c  34.0W   300Mhz  1200Mhz  255%  auto  290.0W    0%   0%    
3    33.0c  39.0W   300Mhz  1200Mhz  255%  auto  290.0W    0%   0%    
4    33.0c  39.0W   300Mhz  1200Mhz  255%  auto  290.0W    0%   0%    
5    32.0c  38.0W   300Mhz  1200Mhz  255%  auto  290.0W    0%   0%    
6    33.0c  34.0W   300Mhz  1200Mhz  255%  auto  290.0W    0%   0%    
7    31.0c  39.0W   300Mhz  1200Mhz  255%  auto  290.0W    0%   0%    
================================================================================
============================= End of ROCm SMI Log ==============================</pre>
<ol type="1" start="3">
<li>Since the compute nodes on the AMD Accelerator Cloud have multiple GPUs per node, and Slurm&#39;s GRES is not configured, it is recommended that you set the HIP_VISIBLE_DEVICES variable to specify which GPU you will run on. The value for HIP_VISIBLE_DEVICES is an integer between 0 and 7 (inclusive); e.g.</li>
</ol>
<pre>export HIP_VISIBLE_DEVICES=3</pre>


      </google-codelab-step>
    
      <google-codelab-step label="Start an Interactive Session on the OCEAN Cluster" duration="5">
        <p>If you plan on using the OCEAN cluster for a portion of this tutorial, you will need to start an interactive session on a compute node to complete this codelab.</p>
<p>The OCEAN cluster comes with the Slurm job scheduler and a set of compute partitions that provide you access to a variety compute platforms. This section will walk you through how to log in to the OCEAN cluster and start an interactive session on a compute node with a GPU.</p>
<ol type="1" start="1">
<li>Open your web browser and navigate to <a href="https://shell.cloud.google.com?show=terminal" target="_blank">https://shell.cloud.google.com?show=terminal</a> . This opens up Google Cloud shell and will be your primary access point to get onto the OCEAN cluster.</li>
</ol>
<aside class="warning"><p><strong>CAUTION: </strong>You must log in with your amd-user*@ocean.waterchange.org account to access the cluster. See <a href="https://www.oshackathon.org/resources/os-hpc-cluster/onboarding" target="_blank">https://www.oshackathon.org/resources/os-hpc-cluster/onboarding</a> for more details.</p>
</aside>
<ol type="1" start="2">
<li>If you have not aligned an ssh key with your account already, you will need to create an SSH key in the Cloud Shell and attach it to your account. This is easily done using ssh-keygen and the gcloud SDK provided for you in the Cloud Shell. <br><br>Run the commands below; at the prompts, accept the default path to store the ssh key pair and set a password to protect the private key.</li>
</ol>
<pre>ssh-keygen -t rsa
gcloud compute os-login ssh-keys add --key-file=${HOME}/.ssh/id_rsa.pub</pre>
<ol type="1" start="3">
<li>Now, ssh to the OCEAN cluster&#39;s login node</li>
</ol>
<pre>ssh ocean.waterchange.org</pre>
<ol type="1" start="4">
<li>Now that you are in the login node, you can start an interactive session on a compute node. When submitting the job, you are required to specify the Quality of Service (QOS) as &#34;amd&#34;.</li>
</ol>
<pre>srun -N1 --gres=gpu:1 --partition=v100 --account=amd --qos=amd --pty /bin/bash</pre>
<aside class="warning"><p><strong>NOTE : </strong>The OCEAN cluster is a cloud-native cluster hosted on Google Cloud Platform. If a compute node is not currently active (denoted by the idle~ state from sinfo), a node will be provisioned automatically on Google Cloud. It may take up to 3 minutes for a node to become active and your interactive session to start.</p>
</aside>


      </google-codelab-step>
    
      <google-codelab-step label="Clone and Run the Demo Application (CPU-Only)" duration="15">
        <p>The demo application provided for this tutorial performs 2-D smoothing operations using a 3x3 gaussian stencil.</p>
<p>In this section, we introduce the demo application and walk through building and verifying the example. It&#39;s important to make sure that the code produces the expected result as we will be using the CPU generated model output to ensure that the solution does not change when we port to the GPU. </p>
<aside class="special"><p><strong>Tip:</strong> In practice, it&#39;s ideal to define tests for all of your routines as standalone (unit-tests) and/or in concert together (integration-tests). These tests would ideally be run regularly during development and with every commit to your code&#39;s repository.</p>
</aside>
<p>This application executes a 2-D smoothing operation on a square grid of points. The program proceeds as follows</p>
<ol type="1" start="1">
<li>Process command line arguments</li>
<li>Allocate memory for smoother class - 5x5 stencil with Gaussian weights</li>
<li>Allocate memory for function and smoothed function</li>
<li>Initialize function on CPU and report function to file</li>
<li>Call smoothing function</li>
<li>Report smoothed function to file</li>
<li>Clear memory</li>
</ol>
<h2 is-upgraded><strong>Code Structure</strong></h2>
<p>This application&#39;s src directory contains the following files</p>
<ol type="1" start="1">
<li><code>smoother.cpp</code> : Defines a simple data structure that stores the smoothing operators weights and the routines for allocating memory, deallocating memory, and executing the smoothing operation.</li>
<li><code>main.cpp</code> : Defines the main program that sets up the 2-D field to be smoothed and managed file IO.</li>
<li><code>Makefile</code> : A simple makefile is to build the application binary <code>smoother</code>.</li>
<li><code>viz.py</code> : A python script for creating plots of the smoother output</li>
</ol>
<h2 is-upgraded><strong>Install and Verify the Application</strong></h2>
<p>To get started, we want to make sure that the application builds and runs on your system using the gcc compiler. </p>
<ol type="1" start="1">
<li>Clone the repository</li>
</ol>
<pre><code>$ git clone https://github.com/fluidnumerics/scientific-computing-edu ~/scientific-computing-edu</code></pre>
<ol type="1" start="2">
<li>Build the smoother application. Keep in mind, the compiler is set to gcc by default in the provided makefile.</li>
</ol>
<aside class="warning"><p><strong>NOTE : </strong>If you are using the OCEAN cluster, you will need to load hipfort into your path using <code>spack load gcc@9.4.0 && spack load hipfort % gcc@9.4.0</code>  before running the make command.</p>
</aside>
<pre><code>$ cd samples/fortran/smoother/src
$ make</code></pre>
<ol type="1" start="3">
<li>Test run the example. The application takes two arguments. The first argument is the number of grid cells, and the second argument is the number of times the smoothing operator is applied.</li>
</ol>
<p>$ ./smoother 1000 1000 100</p>
<h2 is-upgraded><strong>Profile the Application</strong></h2>
<p>Before starting any GPU porting exercise, it is important to profile your application to find hotspots where your application spends most of its time. Further, it is helpful to keep track of the runtime of the routines in your application so that you can later assess whether or not the GPU porting has resulted in improved performance. Ideally, your GPU-Accelerated application should outperform CPU-Only versions of your application when fully subscribed to available CPUs on a compute node.</p>
<aside class="special"><p><strong>Tip:</strong> To obtain a fair comparison between CPU-Only and GPU-Accelerated versions of your application,  you will want to compare the run-time between fully-subscribed CPU-only routines and the GPU-ported routines. </p>
<p>If your application is not parallelized on the CPU, you can estimate the idealized runtime on the CPU by dividing the serial runtime by the number of cores available on your target hardware.</p>
</aside>
<h3 is-upgraded><strong>Create the profile</strong></h3>
<p>In this tutorial, we are going to generate a profile and call graph using gprof. The provided makefile was already configured to create profile output. From here, you just need to use gprof to create the application profile.</p>
<pre><code>$ gprof ./smoother gmon.out</code></pre>
<h3 is-upgraded><strong>Interpret the profile and call tree</strong></h3>
<p><code>gprof</code> provides a flat profile and a summary of your application&#39;s call structure indicating dependencies within your source code as a call tree. A <strong><em>call tree</em></strong> depicts the relationships between routines in your source code. Combining timing information with a call tree will help you plan the order in which you port routines to the GPU.</p>
<p>The first section of the gprof output is the flat-profile. An example flat-profile for the <code>smoother</code> application is given below. The flat-profile provides a list of routines in your application, ordered by the percent time your program spends within those routines from greatest to least. Beneath the flat-profile, gprof provides documentation of each of the columns for your convenience.</p>
<pre><code>  %   cumulative   self              self     total           
 time   seconds   seconds    calls  ms/call  ms/call  name    
 95.24      1.16     1.16       10   116.19   116.19  smoothField
  2.46      1.19     0.03       10     3.00     3.00  resetF
  2.46      1.22     0.03                             main
  0.00      1.22     0.00        1     0.00     0.00  smootherFree
  0.00      1.22     0.00        1     0.00     0.00  smootherInit</code></pre>
<p>Let&#39;s now take a look at at the call tree. This call tree has five entries, one for each routine in our program. The right-most field for each entry indicates the routines that called each routine and that are called by each routine. </p>
<p>For <code>smoother</code>, the first entry shows that main calls <code>smoothField</code>, <code>resetF</code>, <code>smootherInit</code>, and <code>smootherFree</code>. Further, the called column indicates that smoothField and resetF routines are shown to be called 10 times (in this case) by main. The self and children columns indicate that main spends 0.03s executing instructions in main and 1.19s in calling other routines. Further, of those 1.19s, 1.16s are spent in <code>smoothField</code> and 0.03 are spent in <code>resetF</code>. </p>
<pre><code>index % time    self  children    called     name
                                                 &lt;spontaneous&gt;
[1]    100.0    0.03    1.19                 main [1]
                1.16    0.00      10/10          smoothField [2]
                0.03    0.00      10/10          resetF [3]
                0.00    0.00       1/1           smootherInit [5]
                0.00    0.00       1/1           smootherFree [4]
-----------------------------------------------
                1.16    0.00      10/10          main [1]
[2]     95.1    1.16    0.00      10         smoothField [2]
-----------------------------------------------
                0.03    0.00      10/10          main [1]
[3]      2.5    0.03    0.00      10         resetF [3]
-----------------------------------------------
                0.00    0.00       1/1           main [1]
[4]      0.0    0.00    0.00       1         smootherFree [4]
-----------------------------------------------
                0.00    0.00       1/1           main [1]
[5]      0.0    0.00    0.00       1         smootherInit [5]
-----------------------------------------------</code></pre>
<aside class="special"><p><strong>Tip:</strong> You can use the open-source <a href="https://github.com/jrfonseca/gprof2dot" target="_blank">gprof2dot</a> to create visualizations of gprof output to help interpret the profile and call-graph for more complex applications.</p>
</aside>
<h3 is-upgraded><strong>Next steps</strong></h3>
<p>Now that we have a profile and an understanding of the call structure of the application, we can now plan our port to GPUs. Since we will use the AOMP compiler for offloading to GPUs, we want to first modify the Makefile to use the AOMP compiler. Then, we will focus on porting the smoothField routine and the necessary data to the GPU, since smoothField takes up the majority of the run time. </p>
<p>When we port this routine, we will introduce data allocation on the GPU and data copies between CPU and GPU. This data movement may potentially increase the overall application runtime, even if the smoothField routine performs better. In this event, we will then work on minimizing data movements between CPU and GPU. </p>
<aside class="special"><p class="image-container"><img style="width: 231.82px" src="img/ee2462ec9f6dcf9b.png"></p>
<p><strong>Tip:</strong> As a general strategy, it is recommended that you approach GPU porting in small incremental steps. Each step should consist of (1) profiling, (2) planning, (3) implementing planned changes &amp; verifying the application output, and (4) committing the changes to your repository.</p>
</aside>


      </google-codelab-step>
    
      <google-codelab-step label="Moving Data to the GPU with hipfort" duration="20">
        <aside class="special"><p><strong>Tip: </strong>Before getting started in this section, make sure that <code>hipfc</code> is in your path by running <code>hipfc --version</code>. Additionally, if you are on an Nvidia platform, make sure that <code>nvcc</code> is in your path by running <code>nvcc --version</code>.</p>
</aside>
<p>In the <code>smoother</code> application, we have seen that the <code>ApplySmoother</code> routine, called by <code>main</code>, takes up the most time. Looking at the main iteration loop in lines 51-56 in <code>main.F90</code> and the <code>ApplySmoother</code> function in <code>smoother.F90</code>, we see that this function takes in arrays  <code>f</code> and <code>weights</code> and integers <code>nW</code>, <code>nX</code>, and <code>nY</code>. Each call to <code>ApplySmoother</code> returns the <code>smoothF</code> array.</p>
<pre><code> 51     DO iter = 1, nIter
 52 
 53       smoothF = ApplySmoother( f, weights, nW, nX, nY )
 54       CALL ResetF( f, smoothF, nW, nX, nY )
 55 
 56     ENDDO</code></pre>
<p>In order to offload <code>ApplySmoother</code> to the GPU, we will need to copy the <code>f</code> and <code>weights</code> arrays to the GPU. After calling <code>smoothF</code>, we will want to copy <code>smoothF</code> back to the CPU before calling <code>resetF</code>.</p>
<h2 is-upgraded><strong>Saving Reference Output</strong></h2>
<p>Before making changes to the source code, you&#39;ll first want to save the existing output so that you have a reference to compare against later. Having reference output to compare against is critical to verifying that source code changes you make do not change the results.</p>
<p>To do this, make a directory called <code>reference/</code> and copy function.txt and smooth-function.txt to this directory. These files were created during the initial execution of the smoother application in the previous section of this codelab.</p>
<pre><code>$ mkdir ./reference
$ mv function.txt smooth-function.txt ./reference/</code></pre>
<aside class="special"><p><strong>Tip : </strong>When verifying changes you will make later in this codelab, make sure that you use the same input parameters to the <code>smoother</code> application that were used to generate the reference output. </p>
</aside>
<h2 is-upgraded><strong>Copying ALLOCATABLE data to the GPU</strong></h2>
<p>In this section, you will learn how to allocate memory on the GPU and copy data between the host and device using hipfort. You&#39;ll start by inserting calls to allocate and deallocate device memory with hipMalloc and hipFree. Once this is working, you&#39;ll then copy data between the CPU and GPU with hipMemcpy.</p>
<h3 is-upgraded><strong>Allocating Device Memory</strong></h3>
<ol type="1" start="1">
<li>Add <code>USE</code> statements for <code>hipfort</code>, <code>hipfort_check</code>, and <code>ISO_C_BINDING</code> at the top of <code>main.F90</code>. We use ISO_C_BINDING because you will need to declare C-Pointers that you will use to reference memory on the GPU.</li>
</ol>
<pre><code>  PROGRAM main
  
  USE smoother
  USE hipfort
  USE hipfort_check
  
  IMPLICIT NONE</code></pre>
<ol type="1" start="2">
<li>Change the ALLOCATABLE arrays to Fortran POINTER and add declarations for device copies of the <code>f</code>, <code>smoothF</code>, and <code>weights</code> arrays. The device copies can be declared as <code>TYPE(c_ptr)</code> or Fortran <code>POINTER</code>. In this example, we will use Fortran <code>POINTER</code>.</li>
</ol>
<aside class="special"><p><strong>Note: </strong>When working on your own applications, it is recommended to use Fortran <code>POINTER</code>&#39;s, since other API&#39;s, such as MPI, can operate directly with Fortran <code>POINTER</code>&#39;s.</p>
</aside>
<pre><code>  IMPLICIT NONE
    
    INTEGER, PARAMETER :: nW = 2
    INTEGER :: nX, nY, nIter
    REAL(prec), POINTER :: f(:,:)
    REAL(prec), POINTER :: smoothF(:,:)
    REAL(prec), POINTER :: weights(:,:)
    REAL(prec), POINTER :: f_dev(:,:)
    REAL(prec), POINTER :: smoothF_dev(:,:)
    REAL(prec), POINTER :: weights_dev(:,:)
    REAL(prec) :: dx, dy, x, y
    INTEGER :: i, j, iter
</code></pre>
<ol type="1" start="3">
<li>Next, insert calls to <code>hipMalloc</code> wrapped inside of <code>hipCheck</code> to allocate space for <code>f_dev</code>, <code>smoothF_dev</code>, and <code>weights_dev</code>. The hipmalloc routine has an argument called &#34;mold&#34; that you can use to pass the host array to set the size of the device arary. Because of this, you will want to insert these calls <em>after</em> the ALLOCATE statements for <code>f</code>, <code>smoothF</code>, and <code>weights</code>.<br></li>
</ol>
<aside class="special"><p><strong>Note: </strong><code>hipCheck</code> is subroutine provided by the <a href="https://github.com/ROCmSoftwarePlatform/hipfort" target="_blank">hipfort</a> library that will check the error code that is returned by the enclosed HIP API call.</p>
</aside>
<pre><code>      ! Allocate device memory
      CALL hipCheck(hipMalloc(f_dev, mold=f))
      CALL hipCheck(hipMalloc(smoothF_dev, mold=smoothF))
      CALL hipCheck(hipMalloc(weights_dev, mold=weights))</code></pre>
<ol type="1" start="4">
<li>At the end of <code>main.F90</code>, insert calls to <code>hipFree</code> (wrapped inside <code>hipCheck</code>) to free memory held by <code>f_dev</code>, <code>smoothF_dev</code>, and <code>weights_dev</code>.</li>
</ol>
<pre><code>      ! Deallocate GPU memory
      CALL hipCheck( hipFree(f_dev) )
      CALL hipCheck( hipFree(smoothF_dev) )
      CALL hipCheck( hipFree(weights_dev) )</code></pre>
<ol type="1" start="5">
<li>Before going too much further, let&#39;s check to make sure the code compiles and runs with these changes. Starting from the <code>smoother</code> makefile (<code>samples/fortran/smoother/src/Makefile</code>), let&#39;s first add variables for the paths to ROCm and CUDA at the top of the file. These will be needed to reference full paths to the hipfc compiler. <br><br>When setting these variables, we use the ?= relation to allow a user&#39;s environment variables to override these values if desired.</li>
</ol>
<pre><code>ROCM ?= /opt/rocm
CUDA ?= /usr/local/cuda</code></pre>
<p><br>Change the compiler to <code>hipfc</code> and save your changes.</p>
<pre><code>HIPFORT_COMPILER ?= /usr/bin/gfortran
FC ?= $(ROCM)/hipfort/bin/hipfc
FFLAGS=-O0 -g -hipfort-compiler $(HIPFORT_COMPILER)</code></pre>
<aside class="warning"><p><strong>NOTE : </strong>If you are using the OCEAN cluster, we recommend that you load the hipfort % gcc@9.4.0 compiler into your path using <code>spack load hipfort % gcc@9.4.0</code> before running the make command.</p>
<p>Additionally, you may need to the <code>HIPFORT_COMPILER</code> environment variable. </p>
<p><code>export HIPFORT_COMPILER=$(spack location -i gcc@9.4.0)/bin/gfortran</code></p>
</aside>
<aside class="warning"><p><strong>Note: </strong>On the AMD Accelerator Cloud, you will need to load the ROCm module using the command below :</p>
<p><code>module load rocmmod4.5.0</code></p>
</aside>
<ol type="1" start="6">
<li>Remove the <code>*.o</code> files and the <code>smoother</code> binary to ensure a clean build and make a new <code>smoother</code> binary</li>
</ol>
<pre><code>$ make clean &amp;&amp; make smoother</code></pre>
<ol type="1" start="7">
<li>Run <code>smoother</code> with the same input parameters as you did in the previous section and verify the output is unchanged. We use the diff command line utility to compare the output files and the reference files. If there are no differences, diff will produce no output.</li>
</ol>
<pre><code>$ ./smoother 1000 1000 100
$ diff function.txt reference/function.txt
$ diff smooth-function.txt reference.txt</code></pre>
<ol type="1" start="8">
<li>You can verify that data is allocated on the GPU and that data is copied from the CPU to GPU by using a profiler. When running under the profiler on AMD platforms, you should observe three (3) calls to hipMalloc and three (3) calls to hipFree. On Nvidia platforms, you should observe three (3) calls to cudaMalloc and three (3) calls to cudaFree</li>
</ol>
<p><br>For AMD platforms, use <code>rocprof</code> with the <code>--hip-trace</code> flag. Running rocprof will create a file called <code>profile.json</code>. The contents of <code>results.hip_stats.csv</code> will show a summary of calls to <code>hipMalloc</code>, <code>hipMemcpy</code>, and <code>hipFree</code>.<br></p>
<p><br>For Nvidia platforms, use <code>nvprof</code>. When running on Nvidia platforms, you can expect to see calls to <code>cudaMalloc</code>, <code>cudaMemcpy</code>, and <code>cudaFree</code>.</p>
<aside class="special"><p><strong>Tip: </strong>Now that you have made changes to the source code and have verified that the output has not changed, this is a good point to commit your changes to your local git repository. When working on larger codes, testing with every commit and committing frequently can help make a GPU porting project more predictable and manageable.</p>
</aside>
<h2 is-upgraded><strong>Copying Data between the Host and Device</strong></h2>
<p>So far, you have a code that has both CPU (host) and GPU (device) memory. You are now ready to make calls to transfer data between the host and device. </p>
<p>Recall from the first section of this codelab that the <code>ApplySmoother</code> routine is the most expensive routine. Because of this we&#39;ll start by focusing on moving necessary to the GPU before the call to <code>ApplySmoother</code>, and moving data from the GPU after.</p>
<p>To copy data to the GPU (Host To Device), you will use <code>hipMemcpy</code>, e.g.</p>
<pre><code>hipMemcpy( destination, source, hipMemcpyHostToDevice )</code></pre>
<p>The arguments for <code>hipMemcpy</code> are a pointer to where data will be copied to (<code>destination</code>), a pointer to the source of data (<code>source</code>), and an enum provided by the HIP library to indicate the direction of the data transfer. When copying from host-to-device, this last argument is <code>hipMemcpyHostToDevice</code>. When copying from device-to-host, this last argument is <code>hipMemcpyDeviceToHost</code>.</p>
<ol type="1" start="1">
<li>Since the <code>weights</code> array does not change with each iteration, insert a call to <code>hipMemcpy</code> wrapped inside the <code>hipCheck</code> routine to copy weights to weights_dev before the iteration loop starts.</li>
</ol>
<pre>     ! Copy weights to weights_dev
     CALL hipCheck(hipMemcpy(weights_dev, weights, hipMemcpyHostToDevice))</pre>
<ol type="1" start="2">
<li>Just before the call to ApplySmoother, inside the iteration loop, insert calls to hipMemcpy wrapped inside the hipCheck routine to copy f to f_dev.</li>
</ol>
<pre><code>     DO iter = 1, nIter
 
       ! Copy f to f_dev
       CALL hipCheck(hipMemcpy(f_dev, f, hipMemcpyHostToDevice))</code></pre>
<ol type="1" start="3">
<li>When we first port the <code>ApplySmoother</code> routine to the GPU, we will leave resetF on the CPU. Because of this, we will need to have smoothF copied back to the CPU before calling <code>resetF</code>. Now, just after the call to <code>ApplySmoother</code> and before the call to <code>resetF</code>, insert a call to <code>hipMemcpy</code> wrapped inside the <code>hipCheck</code> routine to copy <code>smoothF_dev</code> from the device to the host.</li>
</ol>
<pre><code> 
       smoothF = ApplySmoother( f, weights, nW, nX, nY )
       
       ! Copy smoothF_dev to smoothF
       CALL hipCheck(hipMemcpy(smoothF, smoothF_dev, hipMemcpyDeviceToHost))</code></pre>
<ol type="1" start="4">
<li>Save your changes in main.F90 and recompile the application to make sure there are no syntax errors in your recent code modifications. <br></li>
<li>If you run the application, the output will be incorrect because we are copying <code>smoothF_dev</code> to <code>smoothF</code>, which currently has uninitialized memory. Nonetheless, run the application under a profiler to verify that calls to <code>hipMemcpy</code> ( or <code>cudaMemcpy</code> for Nvidia devices ) are executed.<br><br>For AMD platforms, use <code>rocprof</code> with the <code>--hip-trace</code> flag. Running rocprof will create a file called <code>profile.json</code>. The contents of <code>results.hip_stats.csv</code> will show calls to <code>hipMalloc</code>, <code>hipMemcpy</code>, and <code>hipFree</code>.</li>
</ol>
<pre><code>$ rocprof --hip-trace ./smoother 1000 1000 100
$ cat results.hip_stats.csv 
&#34;Name&#34;,&#34;Calls&#34;,&#34;TotalDurationNs&#34;,&#34;AverageNs&#34;,&#34;Percentage&#34;
hipMemcpy,201,696108877,3463228,99.94123584828755
hipMalloc,3,216944,72314,0.031146925698335683
hipFree,3,192359,64119,0.02761722601411495</code></pre>
<p><br>For Nvidia platforms, use <code>nvprof</code>. At this stage, you should see three calls to <code>cudaMalloc</code>, three calls to <code>cudaFree</code>,  201 calls to <code>cudaMemcpy</code> with 100 calls for <code>DeviceToHost</code>, and 101 calls for <code>HostToDevice</code>.</p>
<pre><code>$ nvprof ./smoother 1000 1000 100
 Info : nX =         1000
 Info : nY =         1000
 Info : nIter =          100
==74347== NVPROF is profiling process 74347, command: ./smoother 1000 1000 100
==74347== Profiling application: ./smoother 1000 1000 100
==74347== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   54.03%  44.247ms       100  442.47us  427.58us  555.39us  [CUDA memcpy DtoH]
                   45.97%  37.645ms       101  372.72us  1.4720us  467.10us  [CUDA memcpy HtoD]
      API calls:   63.95%  171.63ms         3  57.210ms  100.12us  171.43ms  cudaMalloc
                   35.63%  95.610ms       201  475.67us  34.390us  676.19us  cudaMemcpy
                    0.18%  491.33us         3  163.78us  146.94us  172.58us  cudaFree
                    0.15%  407.74us       101  4.0370us     142ns  242.63us  cuDeviceGetAttribute
                    0.06%  166.19us         1  166.19us  166.19us  166.19us  cuDeviceTotalMem
                    0.02%  64.694us         1  64.694us  64.694us  64.694us  cuDeviceGetName
                    0.00%  2.4970us         1  2.4970us  2.4970us  2.4970us  cuDeviceGetPCIBusId
                    0.00%  1.0260us         3     342ns     186ns     625ns  cuDeviceGetCount
                    0.00%     601ns         2     300ns     149ns     452ns  cuDeviceGet
                    0.00%     288ns         1     288ns     288ns     288ns  cuDeviceGetUuid
</code></pre>
<aside class="warning"><p><strong>Caution: </strong>Copying smoothF_dev to <code>smoothF</code> will produce an incorrect result until the <code>ApplySmoother</code> routine is offloaded to the GPU, which we will complete in the next section. This happens because <code>smoothF_dev</code> currently contains uninitialized data that we use to update <code>smoothF</code>.</p>
</aside>
<h2 is-upgraded><strong>Next steps</strong></h2>
<p>At this point, you now have the necessary data declared on the GPU. Additionally, you used <code>hipMemcpy</code> to make the input to <code>ApplySmoother</code> available on the GPU. In the next step, you will create a HIP kernel that will run the <code>ApplySmoother</code> algorithm on the GPU and replace the call to <code>ApplySmoother</code> with a call to launch this kernel.</p>


      </google-codelab-step>
    
      <google-codelab-step label="HIP Kernel Launch Basics" duration="10">
        <p>Routines that are executed on the GPU are typically scheduled to run by issuing instructions from the host (CPU). With both HIP and CUDA, you can use &#34;chevron syntax&#34; to specify the number of threads per block, the number of blocks, the stream ID, and the amount of shared memory per block.</p>
<pre>myGPUFunc &lt;&lt;&lt;BlockPerGrid, ThreadPerBlock,StreamId,ShareMem&gt;&gt;&gt;(int *d_ary, float *d_ary2);</pre>
<p>Keep in mind that HIP kernels must be written in C++. In this section, we&#39;ll cover the basics of writing and launching a HIP kernel in C++ and conclude with a brief overview of how we will launch a HIP kernel from Fortran.</p>
<h2 is-upgraded><strong>Threads, Blocks, and LDS Memory</strong></h2>
<p>When a kernel is launched, all requested threads on the GPU execute the kernel instructions concurrently. How the threads are scheduled to execute the instructions depends on the thread grouping. Threads are grouped into blocks in the HIP and CUDA programming models. Threads within a block are able to share LDS memory and L1 Cache on GPU Compute Units (Streaming Multiprocessor in Nvidia/CUDA terminology).</p>
<p>Within a HIP kernel, you are able to use HIP intrinsics to determine the unique ID of a thread. This allows you to codify memory access patterns for each thread within a kernel and expose SIMD parallelism. These intrinsics are summarized the table below.</p>
<table>
<tr><td colspan="1" rowspan="1"><p><strong>Intrinsic</strong></p>
</td><td colspan="1" rowspan="1"><p><strong>Description</strong></p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>blockIDx.[x,y,z]</p>
</td><td colspan="1" rowspan="1"><p>The block ID in the [x,y,z] grid directions.</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>blockDimx.[x,y,z]</p>
</td><td colspan="1" rowspan="1"><p>The number of threads within each block in the [x,y,z] grid directions.</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>threadIDx.[x,y,z]</p>
</td><td colspan="1" rowspan="1"><p>The local thread ID within a block in the [x,y,z] block directions.</p>
</td></tr>
</table>
<aside class="special"><p><strong>Note:</strong> Block dimensions (threads-per-block) and Grid dimensions (number-of-blocks) are expressed as <a href="https://github.com/ROCm-Developer-Tools/HIP/blob/master/docs/markdown/hip_kernel_language.md#dim3" target="_blank">dim3</a> variables, giving 3 dimensions (x,y,z). </p>
</aside>
<h3 is-upgraded><strong>Example</strong></h3>
<p>To see how this works, consider this simple example kernel that takes in a device array and returns the same array multiplied by two.</p>
<pre><code>__global__ void myKernel(int N, double *d_a) {
  int i = threadIdx.x + blockIdx.x*blockDim.x;
  if (i&lt;N) {
    d_a[i] *= 2.0;
  }
}</code></pre>
<p>In this example, when the kernel is launched, each thread will calculate i based on its thread ID within a block, its block ID, and the number of threads-per-block (<code>blockDimx.x</code>). A conditional is added to ensure that threads only access in-bounds addresses of d_a. Provided this conditional is met for a thread, that thread will double the i-th element of d_a, concurrently with other threads.</p>
<h3 is-upgraded><strong>Best practice</strong></h3>
<p>On AMD GPUs, threads in a block are executed in 64-wide chunks called &#34;wavefronts&#34;. Because of this, it is good practice to make block sizes a multiple of 64 on AMD GPUs.</p>
<p>On Nvidia GPUs, threads in a block are executed in 32-wide chunks called &#34;warps&#34; and it is good practice to make the block size a multiple of 32.</p>
<aside class="warning"><p><strong>Caution: </strong>The number of threads per block, number of blocks, and the amount of local dynamic memory per block are limited; these limits can be found using <a href="https://github.com/RadeonOpenCompute/rocminfo" target="_blank">rocminfo</a> on AMD platforms and the <a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#running-binaries" target="_blank">deviceQuery CUDA-Toolkit example</a> on Nvidia platforms. </p>
</aside>
<h2 is-upgraded><strong>Streams</strong></h2>
<p>In this codelab, we will always set the stream ID to 0. Using multiple streams is useful when you want to run multiple GPU kernels concurrently. This strategy is useful when you are having difficulty keeping GPUs fully subscribed and you have code that has instruction-level parallelism.</p>
<h2 is-upgraded><strong>Launch from Fortran - ISO C Binding</strong></h2>
<p>The ISO C Binding module provides the necessary components to enable Fortran-C interoperability. To call HIP kernels in Fortran, you will need to do the following</p>
<ol type="1" start="1">
<li>Write the GPU accelerated HIP kernel in C++.</li>
<li>Write a wrapper routine in C++ that launches the HIP kernel.</li>
<li>Define a subroutine interface block in Fortran that binds to the wrapper routine in C++.</li>
</ol>
<h3 is-upgraded><strong>Example</strong></h3>
<p>As an example, suppose that you want to offload the following Fortran subroutine to the GPU with HIP.</p>
<pre><code>SUBROUTINE VecAdd( a, b, c, N )
  IMPLICIT NONE
  INTEGER, INTENT(in) :: N
  REAL, INTENT(in) :: a(1:N), b(1:N)
  REAL, INTENT(out) :: c(1:N)

    DO i = 1, N
      c(i) = a(i) + b(i)
    ENDDO
END SUBROUTINE VecAdd</code></pre>
<p>To offload to the GPU, you will first need to write the equivalent HIP kernel in C++,</p>
<pre><code>__global__ void VecAdd_HIP(float *a, float *b, float *c, int N) {
  int i = threadIdx.x + blockIdx.x*blockDim.x;
  if (i&lt;N) {
    c[i] = a[i] + b[i];
  }
}</code></pre>
<p>Then, you will write a wrapper routine in C++ that launches this kernel</p>
<pre><code>extern &#34;C&#34;
{
  void VecAdd_HIPWrapper(float **a, float **b, float **c, int N) {
    VecAdd_HIP&lt;&lt;&lt;dim3(N/64+1,1,1), dim3(64,1,1), 0, 0&gt;&gt;&gt;(*a, *b, *c, N); 
  }
}</code></pre>
<p>In your Fortran source code, usually a module, you will add a subroutine interface block to expose the C++ wrapper routine to Fortran</p>
<pre><code>INTERFACE
  SUBROUTINE VecAdd_HIPWrapper(a, b, c, N) bind(c,name=&#34;VecAdd_HIPWrapper&#34;)
    USE ISO_C_BINDING
    IMPLICIT NONE
      TYPE(c_ptr) :: a, b, c
      INTEGER, VALUE :: N
  END SUBROUTINE VecAdd_HIPWrapper
END INTERFACE</code></pre>
<p>Once these three components are defined, you can then launch the GPU accelerated kernel from Fortran by simply calling <code>VecAdd_HIPWrapper</code>.</p>
<p>In the next section of this codelab, you will use these three steps to offload the ApplySmoother routine to the GPU.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Offload the Smoothing Kernel to the GPU" duration="20">
        <p>In this section, you will offload the <code>ApplySmoother</code> routine to the GPU.</p>
<h2 is-upgraded><strong>Planning the GPU port</strong></h2>
<p>Let&#39;s look at the <code>ApplySmoother</code> subroutine from <code>smoother.F90</code></p>
<pre><code>SUBROUTINE ApplySmoother( f, weights, smoothF, nW, nX, nY )
  IMPLICIT NONE
  REAL(prec), INTENT(in) :: f(1:nX,1:nY)
  REAL(prec), INTENT(in) :: weights(-nW:nW,-nW:nW)
  INTEGER, INTENT(in) :: nW, nX, nY
  REAL(prec), INTENT(inout) :: smoothF(1:nX,1:nY)
  ! Local
  INTEGER :: i, j, ii, jj

    DO j = 1+nW, nY-nW
      DO i = 1+nW, nX-nW
        ! Take the weighted sum of f to compute the smoothF field
        smoothF(i,j) = 0.0_prec
        DO jj = -nW, nW
          DO ii = -nW, nW
            smoothF(i,j) = smoothF(i,j) + f(i+ii,j+jj)*weights(ii,jj)
          ENDDO
        ENDDO
      ENDDO
    ENDDO
END SUBROUTINE ApplySmoother</code></pre>
<p>The outer loops, over i and j, are tightly nested loops over a 2-D grid. The size of these loops are nY-2*nW and nX-2*nW. The values of nX and nY are determined by the user through the first two command line arguments (we have been using 1000 for), and <code>nW</code> is 2 (see the declarations in <code>main.F90</code>). Within the i and j loops, we carry out a reduction operation for smLocal and then assign the value to each element of smoothF.</p>
<p>In the ApplySmoother algorithm, the order in which we execute the i and j loops does not matter. Further, the size of each loop is O(1000) for the example we&#39;re working with. A good strategy for offloading this routine to the GPU is to have each GPU thread execute the instructions within the i and j loops. Ideally, then we want each thread to execute something the following</p>
<pre><code>    real smLocal = 0.0;
    for( int jj=-nW; jj &lt;= nW; jj++ ){
      for( int ii=-nW; ii &lt;= nW; ii++ ){
        iel = (i+ii)+(j+jj)*nX;
        ism = (ii+nW) + (jj+nW)*(2*nW+1);
        smLocal += f[iel]*smoothOperator-&gt;weights[ism];
      }
    }
    iel = i+j*nX;
    smoothF[iel] = smLocal;</code></pre>
<p>Notice now the i and j loops are gone. Within the HIP kernel, we can calculate i and j from <code>threadIdx.[x,y]</code>, <code>blockIdx.[x,y]</code>, and <code>blockDim.[x,y]</code>, assuming that we will launch the kernel with 2-D Grid and Block dimensions. You can use something like the following to calculate i and j.</p>
<pre><code>  size_t i = threadIdx.x + blockIdx.x*blockDim.x;
  size_t j = threadIdx.y + blockIdx.y*blockDim.y;</code></pre>
<aside class="warning"><p><strong>Caution: </strong>In Fortran we can easily control the lower and upper indices for multi-dimensional arrays. When writing HIP kernels, we will use flat 1-D arrays. Additionally, arrays in C use 0-based indexing. </p>
</aside>
<aside class="special"><p><strong>Note:</strong> threadIdx.[x,y] and blockIdx.[x,y] are 0-based indices.</p>
</aside>
<p>Within the main program, you will be able to launch the GPU kernel, but you will need to calculate the Grid and Block Dimensions. For now, let&#39;s assume that the number of threads-per-block in the i and j loop dimensions (x and y directions) is fixed at 16. With the number of threads-per-block (in each direction) chosen, you can calculate the grid dimensions, by requiring the x and y grid dimensions to be greater than or equal to the i and j loop sizes, respectively.</p>
<aside class="special"><p><strong>Note:</strong> In general, the number of threads-per-block will impact the application runtime and you will want to use a suitable profiler to help optimize performance. </p>
<p>On AMD platforms, threads in a block are executed in 64-wide chunks called &#34;wavefronts&#34; and it is good practice is to make the block size a multiple of 64.</p>
<p>On Nvidia platforms, threads in a block are executed in 32-wide chunks called &#34;warps&#34; and it is good practice is to make the block size a multiple of 32.</p>
</aside>
<h2 is-upgraded><strong>Write the HIP Kernels in C++</strong></h2>
<ol type="1" start="1">
<li>You may have noticed that the floating point precision is set in <code>smoother.F90</code> and can be controlled using a C-Preprocessor flag. To extend this feature into the HIP kernels, add a header file, called <code>precision.h</code>. In this header file, define the type <code>real</code> to be <code>double</code> if the <code>DOUBLE_PRECISION</code> preprocessor flag is defined, and <code>float</code> otherwise.</li>
</ol>
<pre><code>#include &lt;float.h&gt;
 
#ifdef DOUBLE_PRECISION
  typedef double real;
#else
  typedef float real;
#endif</code></pre>
<ol type="1" start="2">
<li>Create a new file called <code>smoother_HIP.cpp</code>. This file will contain the C++ source code for the HIP Kernel and the wrapper routine. Start by adding statements to include the HIP runtime and <code>precision.h</code>.</li>
</ol>
<pre><code>#include &#34;precision.h&#34;
#include &lt;hip/hip_runtime.h&gt;</code></pre>
<ol type="1" start="3">
<li>Add a new routine, <code>ApplySmoother_gpu</code>. This routine needs to be of type <code>__global__</code> so that it can be launched on the GPU (device) from the CPU (host) using <code>hipLaunchKernelGGL</code>.</li>
</ol>
<pre><code>__global__ void ApplySmoother_gpu(real *f_dev, real *weights_dev, real *smoothF_dev, int nW, int nX, int nY)
{
  size_t i = threadIdx.x + blockIdx.x*blockDim.x + nW;
  size_t j = threadIdx.y + blockIdx.y*blockDim.y + nW;
  int iel, ism;
  
  if( i &gt;= nW &amp;&amp; i &lt; nX-nW &amp;&amp; j &gt;= nW &amp;&amp; j&lt; nY-nW){
    real smLocal = 0.0;
    for( int jj=-nW; jj &lt;= nW; jj++ ){
      for( int ii=-nW; ii &lt;= nW; ii++ ){
        iel = (i+ii)+(j+jj)*nX;
        ism = (ii+nW) + (jj+nW)*(2*nW+1);
        smLocal += f_dev[iel]*weights_dev[ism];
      }
    }
    iel = i+j*nX;
    smoothF_dev[iel] = smLocal;
  }
}</code></pre>
<aside class="special"><p><strong>Note:</strong> We&#39;ve added <code>nW</code> to <code>i</code> and <code>j</code> since the original loops began at <code>i=nW</code> and <code>j=nW</code>. Also, notice that we&#39;ve added conditionals to safely ensure that threads do not step outside of the memory bounds of <code>f_dev</code>, <code>weights_dev</code>, and <code>smoothF_dev</code>.</p>
</aside>
<ol type="1" start="4">
<li>Next, add a wrapper routine in <code>smoother_HIP.cpp</code> that will launch the HIP kernel. Additionally, you will want to declare the wrapper routine as <code>extern "C"</code>, so that it can be called from Fortran through ISO C Binding. In this routine, you can calculate the grid and block size for the kernel launch. Here, we&#39;ve set the number of threads in the x and y directions to 16 and calculated the grid dimensions by evenly dividing the x and y extents of the outer loops of the original <code>ApplySmoother</code> routine by 16.</li>
</ol>
<pre><code>extern &#34;C&#34;
{
  void ApplySmoother_HIP(real **f_dev, real **weights_dev, real **smoothF_dev, int nW, int nX, int nY)
  {
    int threadsPerBlock = 16;
    int gridDimX = (nX-2*nW)/threadsPerBlock + 1;
    int gridDimY = (nY-2*nW)/threadsPerBlock + 1;
     
    ApplySmoother_gpu&lt;&lt;&lt;dim3(gridDimX,gridDimY,1), dim3(threadsPerBlock,threadsPerBlock,1), 0, 0&gt;&gt;&gt;(*weights_dev, *f_dev, *smoothF_dev, nX, nY, nW);
  } 
} </code></pre>
<h2 is-upgraded><strong>Add Calls from Fortran to Launch the HIP Kernel</strong></h2>
<p>Now that the HIP kernel and a wrapper routine are both defined, we need to modify the Fortran source code to expose the wrapper routine using an INTERFACE block in the smoother.F90 module. Then, in the main.F90 program, you will replace the call to <code>ApplySmoother</code> with a call to <code>ApplySmoother_HIP</code>.</p>
<ol type="1" start="1">
<li>Open the <code>smoother.F90</code> module.</li>
<li>Add an <code>INTERFACE</code> block for the <code>ApplySmoother_HIP</code> routine. This interface block will define the API for the <code>ApplySmoother_HIP</code> routine and will bind this subroutine to <code>ApplySmoother_HIP</code> defined in <code>smoother_HIP.cpp</code>.</li>
</ol>
<pre><code> INTERFACE
   SUBROUTINE ApplySmoother_HIP(f_dev, weights_dev, smoothF_dev, nW, nX, nY) bind(c,name=&#34;ApplySmoother_HIP&#34;)
     USE ISO_C_BINDING
     IMPLICIT NONE
     TYPE(c_ptr) :: f_dev, weights_dev, smoothF_dev
     INTEGER, VALUE :: nW, nX, nY
   END SUBROUTINE ApplySmoother_HIP
 END INTERFACE</code></pre>
<ol type="1" start="3">
<li>Save your changes in <code>smoother.F90</code> and open <code>main.F90</code>.</li>
<li>Navigate down to the <code>ApplySmoother</code> call in the main iteration loop. Replace this call with a call to <code>ApplySmoother_HIP</code>. Replace the input/output variables with their device versions. </li>
</ol>
<aside class="special"><p><strong>Note:</strong> You may have noticed that the Fortran Interface block states that <code>c_ptr</code> types are passed to the <code>ApplySmoother_HIP</code> routine. This is required to be able to launch C routines from Fortran. To &#34;convert&#34; the Fortran pointer to a <code>c_ptr</code>, you can use the <code>c_loc</code> intrinsic from <code>ISO_C_BINDING</code>.</p>
</aside>
<pre><code>CALL ApplySmoother_HIP( c_loc(f_dev), c_loc(weights_dev), c_loc(smoothF_dev), nW, nX, nY )</code></pre>
<ol type="1" start="5">
<li>Save your changes in <code>main.F90</code>.</li>
</ol>
<h2 is-upgraded><strong>Update the Makefile</strong></h2>
<ol type="1" start="1">
<li>Open the <code>Makefile</code>.</li>
<li>Add instructions to compile <code>smoother_HIP.cpp</code> to an object file. Note that you can continue to use <code>hipfc</code> as the compiler since hipfc will detect the <code>.cpp</code> file extension and switch to using <code>hipcc</code> automatically. Notice however, that you can use the <code>CFLAGS</code> variable to pass compilation flags specific to compiling this C++ file.</li>
</ol>
<pre><code>smoother_HIP.o : smoother_HIP.cpp
     $(FC) $(CFLAGS) -c smoother_HIP.cpp  -o $@</code></pre>
<ol type="1" start="3">
<li>Add <code>smoother_HIP.o</code> dependencies on the <code>smoother.o</code> and <code>smoother</code> targets.</li>
</ol>
<pre><code>smoother: main.o smoother.o smoother_HIP.o
    ${FC} ${FFLAGS} *.o -o $@

main.o : main.F90 smoother.o
    $(FC) $(FFLAGS) -c main.F90  -o $@

smoother.o : smoother.F90 smoother_HIP.o
    $(FC) $(FFLAGS) -c smoother.F90  -o $@</code></pre>
<ol type="1" start="4">
<li>Save the Makefile and rebuild the application.</li>
</ol>
<aside class="warning"><p><strong>Caution: </strong>In order to obtain bit-for-bit correctness on Nvidia Platforms, you must add the <code>-fmad=false</code> flag to the <code>CFLAGS</code> variable in the Makefile. This flag disables fused multiply-add operations. Fused Multiply-Add (FMA) operations on the GPU result in improved performance but can cause a difference in the computed solution between <code>ApplySmoother</code> and <code>ApplySmoother_HIP</code>.</p>
</aside>
<h2 is-upgraded><strong>Profile the application</strong></h2>
<p>You can verify that data is allocated on the GPU and that data is copied from the CPU to GPU by using a profiler.<br><br>To profile, use <code>rocprof</code> with the <code>--hip-trace on</code> and <code>--stats</code> flags. Running <code>rocprof</code> will create a file called <code>results.json</code> that contains the data for a trace profile. Additionally, <code>results.stats.csv</code> and <code>results.hip-stats.csv</code> will contain hotspot analysis for HIP kernels and HIP API calls, respectively.</p>
<pre><code>$ rocprof --stats ./smoother 1000 1000 100</code></pre>
<aside class="special"><p><strong>Tip:</strong> You can use <a href="https://www.chromium.org/developers/how-tos/trace-event-profiling-tool" target="_blank">Google Chrome Tracing</a> to visualize the results.json trace profile. Simply open the Google Chrome web browser and navigate to chrome://tracing and upload results.json. </p>
<p class="image-container"><img style="width: 610.00px" src="img/3682f6a6d97594e4.png"></p>
</aside>
<aside class="special"><p><strong>Tip:</strong> Once you have verified your results, make sure that you commit your changes to your local git repository.</p>
</aside>
<h2 is-upgraded><strong>Next steps</strong></h2>
<p>Congratulations! So far, you&#39;ve learned how to allocate and manage memory on the GPU and how to launch a GPU kernel from Fortran using hipfort, HIP, and ISO C Binding. </p>
<p>Right now, we have the code in a state where, every iteration, data is copied to the GPU before calling <code>ApplySmoother_gpu</code> and from the GPU after calling A<code>pplySmoother_gpu</code>. This situation happens quite often in the early stages of porting to the GPU.</p>
<p>The next step in this codelab is to offload the <code>ResetF</code> routine to the GPU, even though it does not take up a lot of time. We want to offload it to the GPU so that we can move the <code>hipMemcpy</code> calls outside of the iteration loop in main and reduce the number of times data is transmitted across the PCI bus.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Offload the resetF Kernel to the GPU" duration="10">
        <p>In this section, we are going to offload the <code>ResetF</code> routine in <code>smoother.F90</code> to the GPU so that we can migrate <code>hipMemcpy</code> calls outside of the iteration loop in <code>main.F90</code>. By this point, you have worked through the mechanics for porting a routine to the GPU  :</p>
<ol type="1" start="1">
<li>Add a HIP kernel in C++</li>
<li>Add a wrapper routine in C++ to launch the HIP kernel</li>
<li>Add an interface block in Fortran</li>
<li>Replace the subroutine/function call with a call to the wrapper routine</li>
</ol>
<p>In this section, we&#39;ll repeat these steps for the <code>ResetF</code> routine. Additionally, we&#39;ll take an extra step, where we push the <code>hipMemcpy</code> calls outside of the main iteration loop, in order to reduce the number of data transfers between the CPU and GPU.</p>
<h2 is-upgraded><strong>Write the HIP Kernels in C++</strong></h2>
<ol type="1" start="1">
<li>Open <code>smoother_HIP.cpp</code></li>
<li>Add a new routine, <code>ResetF_gpu</code>. This routine needs to be of type <code>__global__</code> so that it can be launched on the GPU (device) from the CPU (host) using <code>hipLaunchKernelGGL</code>.</li>
</ol>
<pre><code>__global__ void ResetF_gpu(real *f_dev, real *smoothF_dev, int nW, int nX, int nY)
{
  size_t i = threadIdx.x + blockIdx.x*blockDim.x + nW;
  size_t j = threadIdx.y + blockIdx.y*blockDim.y + nW;
  int iel = i + nX*j;
  if( i &gt;= nW &amp;&amp; i &lt; nX-nW &amp;&amp; j &gt;= nW &amp;&amp; j&lt; nY-nW){
    f_dev[iel] = smoothF_dev[iel];
  }
}</code></pre>
<ol type="1" start="3">
<li>Next, add a wrapper routine that will launch the HIP kernel. As before, you will want to declare the wrapper routine as <code>extern "C"</code>, so that it can be called from Fortran through ISO C Binding. In this routine, you can calculate the grid and block size for the kernel launch. Here, we&#39;ve set the number of threads in the x and y directions to 16 and calculated the grid dimensions by evenly dividing the x and y extents of the outer loops of the original <code>ResetF</code> routine by 16.</li>
</ol>
<pre><code>extern &#34;C&#34;
{ 
  void ResetF_HIP(real **f_dev, real **smoothF_dev, int nW, int nX, int nY)
  { 
    int threadsPerBlock = 16;
    int gridDimX = (nX-2*nW)/threadsPerBlock + 1;
    int gridDimY = (nY-2*nW)/threadsPerBlock + 1;

    hipLaunchKernelGGL((ResetF_gpu), dim3(gridDimX,gridDimY,1), dim3(threadsPerBlock,threadsPerBlock,1), 0, 0, *f_dev, *smoothF_dev, nW, nX, nY);
  }
}</code></pre>
<ol type="1" start="4">
<li>Save smoother_HIP.cpp</li>
</ol>
<h2 is-upgraded><strong>Add Calls from Fortran to Launch the HIP Kernel</strong></h2>
<p>As with the <code>ApplySmoother_HIP</code> routine, you will add an INTERFACE block in the <code>smoother.F90</code> module for the <code>ResetF_HIP</code>. Then, in the <code>main.F90</code> program, you will replace the call to <code>ResetF</code> with a call to <code>ResetF_HIP</code>.</p>
<ol type="1" start="1">
<li>Open the <code>smoother.F90</code> module.</li>
<li>Add an <code>INTERFACE</code> block for the <code>ResetF_HIP</code> routine. This interface block will define the API for the <code>ResetF_HIP</code> routine and will bind this subroutine to <code>ResetF_HIP</code> defined in <code>smoother_HIP.cpp</code>.</li>
</ol>
<pre><code> INTERFACE
   SUBROUTINE ResetF_HIP(f_dev, smoothF_dev, nW, nX, nY) bind(c,name=&#34;ResetF_HIP&#34;)
     USE ISO_C_BINDING
     IMPLICIT NONE
     TYPE(c_ptr) :: f_dev, smoothF_dev
     INTEGER, VALUE :: nW, nX, nY
   END SUBROUTINE ResetF_HIP
 END INTERFACE</code></pre>
<ol type="1" start="3">
<li>Save your changes in <code>smoother.F90</code> and open <code>main.F90</code>.</li>
<li>Navigate down to the <code>ResetF</code> call in the main iteration loop. Replace this call with a call to <code>ResetF_HIP</code>. Replace the input/output variables with their device versions.</li>
</ol>
<pre><code>CALL ResetF_HIP( c_loc(f_dev), c_loc(smoothF_dev), nW, nX, nY )</code></pre>
<ol type="1" start="5">
<li>With <code>ResetF_HIP</code> in place, we can now move the <code>hipMemcpy</code> calls outside of the main iteration loop. Move the call to copy <code>f</code> to <code>f_dev</code> to just before the do loop. To obtain the correct result, you will also need to add a call to copy <code>smoothF</code> to <code>smoothF_dev</code> before the iteration loop. Then, move the call to copy <code>smoothF_dev</code> to <code>smoothF</code> after the do loop.</li>
</ol>
<pre><code>CALL hipCheck(hipMemcpy(smoothF_dev, smoothF, hipMemcpyHostToDevice))
CALL hipCheck(hipMemcpy(f_dev, f, hipMemcpyHostToDevice))
DO iter = 1, nIter
  CALL ApplySmoother_HIP( c_loc(f_dev), c_loc(weights_dev), c_loc(smoothF_dev), nW, nX, nY )
  CALL ResetF_HIP( c_loc(f_dev), c_loc(smoothF_dev), nW, nX, nY )
ENDDO
CALL hipCheck(hipMemcpy(smoothF, smoothF_dev, hipMemcpyDeviceToHost))</code></pre>
<ol type="1" start="6">
<li>Save your changes in <code>main.F90</code>. Rebuild and re-run the smoother application with the same parameters as before. Verify that the solution has remain unchanged.</li>
</ol>
<aside class="special"><p><strong>Tip:</strong> Once you have verified your results, make sure that you commit your changes to your local git repository.</p>
</aside>


      </google-codelab-step>
    
      <google-codelab-step label="Congratulations" duration="0">
        <p>In this codelab, you learned how to port serial CPU-only routines in Fortran to GPUs using <a href="https://github.com/ROCmSoftwarePlatform/hipfort" target="_blank">hipfort</a>, HIP, and ISO C Binding. To do this, you used Fortrna <code>POINTER</code>&#39;s to create device copies of CPU arrays and learned how to copy data from the CPU to the GPU and vice versa with <code>hipMemcpy</code>. You also learned how to write HIP kernels in C++, expose them to Fortran, and launch them from the host within Fortran source code. </p>
<p>In the process of doing this, you practiced a strategy for porting to GPUs that included the following steps to make incremental changes to your own source code :</p>
<ol type="1" start="1">
<li>Profile - Find out the hotspots in your code and understand the dependencies with other routines</li>
<li>Plan - Determine what routine you want to port, what data needs to be present on the GPU, and what data needs to be copied back to the CPU after execution</li>
<li>Implement &amp; Verify - Create the necessary device data, insert the appropriate hipMemcpy calls, write an equivalent GPU kernel, and use hipLaunchKernelGGL to launch the GPU kernel. Run your application&#39;s tests and verify the results are correct. Check with a profiler that the new routine and the necessary hipMemCpy calls are being executed.</li>
<li>Commit - Once you have verified correctness and the expected behavior, commit your changes and start the process over again.</li>
</ol>
<h2 is-upgraded><strong>Provide Feedback</strong></h2>
<p>If you have any questions, comments, or feedback that can help improve this codelab, you can reach out to <a href="mailto:support@fluidnumerics.com" target="_blank">support@fluidnumerics.com</a> </p>
<h2 is-upgraded>Further reading</h2>
<ul>
<li><a href="https://rocmdocs.amd.com/en/latest/Programming_Guides/HIP-GUIDE.html" target="_blank">HIP Programming Guide</a></li>
<li><a href="https://rocmdocs.amd.com/en/latest/Programming_Guides/Programming-Guides.html#hip-documentation" target="_blank">HIP Documentation</a></li>
<li><a href="https://rocmdocs.amd.com/en/latest/" target="_blank">About AMD ROCm</a></li>
<li><a href="https://rocm-developer-tools.github.io/HIP/" target="_blank">HIP API Documentation</a></li>
<li><a href="https://docs.nvidia.com/cuda/floating-point/index.html#fused-multiply-add-fma" target="_blank">Fused Multiply-Add on Nvidia hardware</a></li>
</ul>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/codelab-elements/native-shim.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/prettify.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>

</body>
</html>
