
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Building a basic GPU accelerated application with HIP in C/C&#43;&#43;</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/codelab-elements/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab codelab-gaid="TO DO"
                  id="build-a-gpu-app-hip-c"
                  title="Building a basic GPU accelerated application with HIP in C/C&#43;&#43;"
                  environment="web"
                  feedback-link="https://octoskelo.atlassian.net/servicedesk/customer/portals">
    
      <google-codelab-step label="Introduction" duration="0">
        <p><strong>Last Updated:</strong> 2020-18-09</p>
<p>Over the last few decades, there has been increased interest in using Graphics Processing Units (GPUs) to perform general purpose computing tasks. This practice is often referred to as General Purpose GPU (GPGPU) programming. Using GPUs for general purpose computing tasks gained attention primarily due to the inherent scale of parallelism within GPU hardware that enables faster computation and a reduction in the time-to-solution.</p>
<p>Since GPUs were designed for handling graphics rendering tasks, implementing general purpose routines, like those for numerically solving partial differential equation or optimizing the weights in a neural network, was time-consuming and often error-prone. The continued interest and success of GPGPU computing led to the development of more user-friendly application programming interfaces (APIs) that allow developers to focus more attention on implementing algorithms using the syntax of compiled languages they are more familiar with, rather than thinking about how to express their algorithm in terms of graphics operations.</p>
<p>Before diving into GPU programming APIs and how they can help you accelerate scientific applications, let&#39;s first discuss the basics of modern GPU accelerated compute platforms to help you better understand the software development problems they help solve.</p>
<h2 is-upgraded><strong>Basics of GPU Accelerated Platforms</strong></h2>
<p>A GPU is an additional hardware component that can perform operations alongside a CPU. GPUs are either integrated into the motherboard or silicon dye alongside a CPU, or are made available through a dedicated interconnect, called the Peripheral Component Interconnect (PCI). The PCI is a physical hardware component that allows data to be transmitted between the CPU and GPU.</p>
<p class="image-container"><img style="width: 348.50px" src="img/864a4a878a132b1f.png"></p>
<p>On GPU-Accelerated High Performance Computing platforms, you will primarily encounter servers with one or more dedicated GPUs. Dedicated GPUs have an isolated set of compute cores and their own memory space, distinct from the CPU and the CPU&#39;s memory space. The figure above illustrates simple conceptual model of a server with a CPU connected to a single GPU. This conceptual model is purposefully simplified to highlight the first hurdle that all new GPU developers must overcome : managing CPU and GPU memory spaces.</p>
<p>On most modern GPU accelerated platforms, migrating data between the CPU and GPU can be a bottleneck. This is caused by limits in the PCI Bus peak bandwidth. Because of this, developers must be mindful to minimize the amount of data transfer between CPU and GPU for optimal performance.</p>
<p>A more subtle aspect of GPU programming, driven by the fact that a GPU is a separate hardware component from the CPU, is the potential for asynchronous activities between the CPU and GPU. When developing a GPU accelerated application, kernels that can execute on the GPU are scheduled for execution by the CPU. Most modern GPU programming APIs provide calls that can force the issuing CPU process to stop and wait for the GPU kernel execution. Further, when a CPU issues multiple kernel execution instructions, these APIs can allow for serialized or asynchronous executions.</p>
<h3 is-upgraded>GPU Hardware</h3>
<p>GPUs for high performance computing are available from three different vendors : AMD, Nvidia, and Intel. Currently, each has their own terminology for describing the architecture and microarchitecture. We&#39;ll briefly describe a conceptual model of a GPU and relate terminology between vendors.<img style="width: 600.00px" src="img/c4854d064849fbc1.png"></p>
<p>In general, GPUs are comprised of a number of compute units (AMD) or streaming multiprocessors (Nvidia), &#34;Global&#34; GPU RAM, and a Work Group Distributor (AMD) or Workload Manager (Nvidia). On Nvidia hardware, the Workload Manager schedules work to the streaming multiprocessors, which have a Same-Instruction-Multiple-Thread (SIMT) scheduler, Cache memory, registers, and a set of CUDA Cores. On AMD Hardware, the Work Group Distributor schedules work across the compute units, which each have a scheduler, local data share, L1 Cache, a mix of scalar and vector registers, and a Vector Arithmetic Logic Unit (ALU). </p>
<p>GPU models are distinguished based on their microarchitecture and other characteristics, such as the number of compute units, PCI compatibility, memory and compute clock frequencies, and global memory size. For AMD GPUs, the microarchitecture refers to the architecture of the compute units.</p>
<p>Below is a conceptual diagram of a single compute unit in AMD&#39;s Vega 20 micro-architecture. This micro-architecture is at the core of the Radeon Instinct MI50 &amp; MI60 GPUs and the <a href="https://www.amd.com/en/products/exascale-era%5C" target="_blank">Department of Energy&#39;s newest exascale systems</a>, <a href="https://www.llnl.gov/news/llnl-and-hpe-partner-amd-el-capitan-projected-worlds-fastest-supercomputer" target="_blank">El Capitan</a> and <a href="https://www.olcf.ornl.gov/frontier/" target="_blank">Frontier</a>. On AMD GPUs, each compute unit has 64 Vector ALU&#39;s.<img style="width: 624.00px" src="img/605439049bb81405.jpeg"></p>
<p>The table below summarizes some of the characteristics of a few of the <a href="https://www.amd.com/en/graphics/servers-radeon-instinct-mi" target="_blank">latest lineup of AMD Radeon Instinct GPUs</a>. In this table, we are showing GPUs with varying microarchitecture, number of compute units, and global GPU memory size. The GPU&#39;s memory clock rate and compute clock rate, together with the type of memory and the number of Vector ALU&#39;s dictate the peak performance and memory bandwidth.</p>
<table>
<tr><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"><p><a href="https://www.amd.com/en/products/professional-graphics/instinct-mi50-32gb" target="_blank"><strong>MI50</strong></a></p>
</td><td colspan="1" rowspan="1"><p><a href="https://www.amd.com/en/products/professional-graphics/instinct-mi25" target="_blank"><strong>MI25</strong></a></p>
</td><td colspan="1" rowspan="1"><p><a href="https://www.amd.com/en/products/professional-graphics/instinct-mi8" target="_blank"><strong>MI8</strong></a></p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p><strong>Microarchitecture</strong></p>
</td><td colspan="1" rowspan="1"><p>Vega20</p>
</td><td colspan="1" rowspan="1"><p>Vega10</p>
</td><td colspan="1" rowspan="1"><p>Fiji</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p><strong>Compute Units</strong></p>
</td><td colspan="1" rowspan="1"><p>60</p>
</td><td colspan="1" rowspan="1"><p>64</p>
</td><td colspan="1" rowspan="1"><p>64</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p><strong>Peak FP16 </strong></p>
</td><td colspan="1" rowspan="1"><p>26.5 TFLOPS</p>
</td><td colspan="1" rowspan="1"><p>24.6 TFLOPS</p>
</td><td colspan="1" rowspan="1"><p>8.19 TFLOPS</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p><strong>Peak FP32 </strong></p>
</td><td colspan="1" rowspan="1"><p>13.3 TFLOPS</p>
</td><td colspan="1" rowspan="1"><p>12.29 TFLOPS</p>
</td><td colspan="1" rowspan="1"><p>8.19 TFLOPS</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p><strong>Peak FP64 </strong></p>
</td><td colspan="1" rowspan="1"><p>6.6 TFLOPS</p>
</td><td colspan="1" rowspan="1"><p>768 GFLOPS</p>
</td><td colspan="1" rowspan="1"><p>512 GFLOPS</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p><strong>Memory Size</strong></p>
</td><td colspan="1" rowspan="1"><p>16-32 GB (HBM2)</p>
</td><td colspan="1" rowspan="1"><p>16 GB (HBM2)</p>
</td><td colspan="1" rowspan="1"><p>4 GB (HBM)</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p><strong>Memory Bandwidth</strong></p>
</td><td colspan="1" rowspan="1"><p>1 TB/s</p>
</td><td colspan="1" rowspan="1"><p>484 GB/s</p>
</td><td colspan="1" rowspan="1"><p>512 GB/s</p>
</td></tr>
</table>
<p>Now that you have some awareness of GPU hardware, let&#39;s talk about how we program GPUs to accelerate scientific applications.</p>
<h2 is-upgraded><strong>GPU Programming APIs</strong></h2>
<p>In general, a GPU programming API must provide routines that developers can leverage to allocate and deallocate memory on the GPU, copy memory between the CPU and GPU, and control kernel execution. GPU programming APIs can be classified into two categories</p>
<ol type="1" start="1">
<li>Directive-Based</li>
<li>Kernel-Based</li>
</ol>
<p>When programming with Directive-Based APIs, developers will provide &#34;hints&#34; to the compiler about how to offload sections of code to the GPU. In this approach, the compiler will then generate code for allocating/deallocating memory, copying memory between host and device, and how to parallelize sections of code. This method of GPU programming has the benefit of being able to start running on GPUs quickly with little effort. Additionally, management of CPU and GPU memory is handled &#34;behind-the-scenes&#34; by the compiler and can help limit code complexity. In this case, compilers will more often make decisions that ensure correctness, rather than optimize performance. Because of this, performance tuning often requires verbose compiler hints to limit superfluous data transfer between CPU and GPU and sometimes require alteration of the CPU code.</p>
<p>When programming with Kernel-Based APIs, developers are solely responsible for creating and managing both CPU and GPU memory spaces. Additionally, developers must write compute kernels that are consistent with their CPU counterparts and issue explicit calls to launch routines when needed. While this approach increases code complexity and has a higher barrier to entry than Directive-Based approaches, the developer has precise control over the performance of GPU kernels. Additionally, developers can control when data transfers between CPU and GPU occur, allowing for a clear path to minimize time spent crossing the PCI Bus.</p>
<p>The table below provides a breakdown of popular GPU programming APIs, their type, which compilers expose the API, and which GPU platforms the API allows you to program for.  It&#39;s important to keep in mind that directive-based APIs yield varied performance across compilers. Further, Fortran compilers that are 2003 compliant and above are able to leverage <a href="http://fortranwiki.org/fortran/show/iso_c_binding" target="_blank">ISO_C_BINDING</a> to expose C/C++ routines that can be called from Fortran source code, allowing C/C++ APIs to be made available in Fortran through C-interoperability.</p>
<table>
<tr><td colspan="1" rowspan="1"><p><strong>API </strong></p>
</td><td colspan="1" rowspan="1"><p><strong>Type</strong></p>
</td><td colspan="1" rowspan="1"><p><strong>Compiler Support</strong></p>
</td><td colspan="1" rowspan="1"><p><strong>Platforms</strong></p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p><a href="https://github.com/ROCm-Developer-Tools/HIP" target="_blank">HIP</a> &amp; <a href="https://github.com/ROCmSoftwarePlatform/hipfort" target="_blank">hipfort</a></p>
</td><td colspan="1" rowspan="1"><p>Kernel</p>
</td><td colspan="1" rowspan="1"><p>Hipcc (hcc/nvcc)</p>
</td><td colspan="1" rowspan="1"><p>AMD, Nvidia</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p><a href="https://www.openmp.org/specifications/" target="_blank">OpenMP (v5.0)</a></p>
</td><td colspan="1" rowspan="1"><p>Directive</p>
</td><td colspan="1" rowspan="1"><p>AOMP (Clang/Flang), GCC 10, XL</p>
</td><td colspan="1" rowspan="1"><p>AMD, Nvidia</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>CUDA</p>
</td><td colspan="1" rowspan="1"><p>Kernel</p>
</td><td colspan="1" rowspan="1"><p>nvcc</p>
</td><td colspan="1" rowspan="1"><p>Nvidia</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>CUDA-Fortran</p>
</td><td colspan="1" rowspan="1"><p>Kernel</p>
</td><td colspan="1" rowspan="1"><p>PGI</p>
</td><td colspan="1" rowspan="1"><p>Nvidia</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>OpenACC</p>
</td><td colspan="1" rowspan="1"><p>Directive</p>
</td><td colspan="1" rowspan="1"><p>GCC 9, PGI, XL</p>
</td><td colspan="1" rowspan="1"><p>Nvidia</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p><a href="https://www.khronos.org/opencl/" target="_blank">OpenCL</a></p>
</td><td colspan="1" rowspan="1"><p>Kernel</p>
</td><td colspan="1" rowspan="1"><p>All</p>
</td><td colspan="1" rowspan="1"><p>All</p>
</td></tr>
</table>
<h3 is-upgraded><strong>ROCm, HIP, and OpenMP : Portable, Open-Source Platforms for GPU Acceleration</strong></h3>
<p>AMD, Nvidia, and Intel all design and manufacture GPUs for High Performance Computing. Currently, there is no unified machine code for GPUs that all vendors currently support on the hardware they produce. This has resulted in portability issues and the common &#34;vendor-lock&#34; problem, where HPC developers spend a significant amount of effort to port their application to a specific GPU and then lose the ability to easily transition to other hardware.</p>
<p>As we have just shown, there are a number of APIs available that support GPGPU programming. Currently, this ecosystem is at a turning point where APIs are shifting towards meeting open-source and portability standards that enable developers to leverage GPU hardware from multiple vendors and even multi-core CPU platforms all with the same code. </p>
<h4 is-upgraded>ROCm</h4>
<p>AMD is currently leading this effort through its <a href="https://www.amd.com/en/graphics/servers-solutions-rocm" target="_blank">ROCm platform</a>. ROCm is AMD&#39;s open source platform for GPU accelerated computing that covers everything from the device driver and runtimes, to compilers, programming models and libraries. It also supports different frameworks and applications and comes with a complete set of developer tools for debugging and profiling your application to help you get the best possible performance.</p>
<p>The diagram below summarizes the ROCm ecosystem that helps bridge the gap between HPC and Machine Learning applications and the variety of compute hardware targets, including GPUs.<img style="width: 624.00px" src="img/1a114e258c2c2c69.png"></p>
<h4 is-upgraded>HIP</h4>
<p>For Kernel-based GPU programming, ROCm includes the <a href="https://rocmdocs.amd.com/en/latest/Installation_Guide/HIP.html" target="_blank">Heterogeneous-Compute Interface for Portability (HIP)</a> and OpenCL. The Heterogeneous Interface for Portability (HIP) is AMD&#39;s dedicated GPU programming environment for designing high performance kernels on GPU hardware. AMD provides hipify tools that will convert CUDA to HIP, enhancing the performance portability of your GPU accelerated applications. The interface design of the API allows your new single source application to be compiled to target either AMD or NV hardware.</p>
<p>HIP is a C++ dialect, similar to CUDA, that allows for programming and AMD and Nvidia GPUs. HIP maintainers have plans to support Intel (XE) GPUs in future releases. The latest version of the ROCm package, now includes <a href="https://github.com/ROCmSoftwarePlatform/hipfort" target="_blank">hipfort</a>, a Fortran interface that exposes the HIP API through ISO C Binding. <a href="https://rocmdocs.amd.com/en/latest/Programming_Guides/Opencl-programming-guide.html" target="_blank">OpenCL</a> is framework that is available through a C runtime API and is supported by AMD, Intel, and Nvidia GPUs and x86 CPUs. The ROCm platform provides an OpenCL runtime environment necessary for building portable, parallel applications that run on a variety of platforms.</p>
<h4 is-upgraded>OpenMP 5.0</h4>
<p>For Directive-based GPU programming, ROCm includes the <a href="https://github.com/ROCm-Developer-Tools/aomp" target="_blank">AOMP compilers</a> for C/C++ and Fortran. The AOMP compilers are an extension of the LLVM-based Clang and Flang compilers that support the OpenMP 5.0 standard for multi-core CPU and GPU programming on both AMD and Nvidia GPUs.</p>
<h4 is-upgraded>GPU Accelerated Libraries</h4>
<p>In addition to the programming APIs, <a href="https://rocmdocs.amd.com/en/latest/ROCm_Libraries/ROCm_Libraries.html" target="_blank">ROCm includes portable accelerated HPC and Machine Learning libraries</a>, such as  <a href="https://rocmdocs.amd.com/en/latest/ROCm_Libraries/ROCm_Libraries.html#rocblas" target="_blank">rocBLAS</a>, <a href="https://rocmdocs.amd.com/en/latest/ROCm_Libraries/ROCm_Libraries.html#rocfft" target="_blank">rocFFT</a>, <a href="https://rocmdocs.amd.com/en/latest/ROCm_Libraries/ROCm_Libraries.html#rocthrust" target="_blank">rocThrust</a>, <a href="https://rocmdocs.amd.com/en/latest/ROCm_Libraries/ROCm_Libraries.html#hipsparse" target="_blank">rocSparse</a>, <a href="https://rocmdocs.amd.com/en/latest/Deep_learning/Deep-learning.html#tensorflow" target="_blank">Tensorflow</a>, <a href="https://rocmdocs.amd.com/en/latest/Deep_learning/Deep-learning.html#pytorch" target="_blank">PyTorch</a>, <a href="https://rocmdocs.amd.com/en/latest/Deep_learning/Deep-learning.html#miopen" target="_blank">MIOpen</a>, and many others. All of these tools are provided under open-source licensing and made freely available to help you accelerate your time-to-science in a community driven ecosystem. These libraries are beneficial when you want to quickly and optimally leverage GPUs, without having to write GPU kernels yourself.</p>
<h2 is-upgraded><strong>What you will build</strong></h2>
<p>In this codelab, we will focus on how to accelerate an application in C with HIP. You are going to work through transitioning a serial CPU-only mini-application to a portable GPU accelerated application, using AMD&#39;s HIP. </p>
<h2 is-upgraded><strong>What you will learn</strong></h2>
<ul>
<li>How to develop a GPU porting strategy using application profiles and call graphs.</li>
<li>How to manage GPU memory with HIP.</li>
<li>How to launch GPU accelerated kernels with HIP.</li>
<li>How to build GPU accelerated C/C++ applications for AMD and Nvidia platforms with a simple Makefile.</li>
<li>How to verify GPU memory allocation and kernel execution with the rocprof profiler.</li>
</ul>
<h2 is-upgraded><strong>What you will need</strong></h2>
<ul>
<li>A compute platform with AMD or Nvidia GPU(s)</li>
<li>Linux operating system (e.g. Debian, Ubuntu, CentOS, or RHEL)</li>
<li>Working installation of <a href="https://rocm-documentation.readthedocs.io/en/latest/Installation_Guide/Installation-Guide.html" target="_blank">ROCm-dev</a> ( v3.3 )</li>
<li>Basic Command-Line Linux Experience</li>
<li>Working C or C++ compiler</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Clone and Run the Demo Application (CPU-Only)" duration="15">
        <p>In this section, we introduce the demo application and walk through building and verifying the example. It&#39;s important to make sure that the code produces the expected result as we will be using the CPU generated model output to ensure that the solution does not change when we port to the GPU. </p>
<aside class="special"><p><strong>Tip:</strong> In practice, it&#39;s ideal to define tests for all of your routines as standalone (unit-tests) and/or in concert together (integration-tests). These tests would ideally be run regularly during development and with every commit to your code&#39;s repository.</p>
</aside>
<p>This application executes a 2-D smoothing operation on a square grid of points. The program proceeds as follows</p>
<ol type="1" start="1">
<li>Process command line arguments</li>
<li>Allocate memory for smoother class - 5x5 stencil with Gaussian weights</li>
<li>Allocate memory for function and smoothed function</li>
<li>Initialize function on CPU and report function to file</li>
<li>Call smoothing function</li>
<li>Report smoothed function to file</li>
<li>Clear memory</li>
</ol>
<h2 is-upgraded><strong>Code Structure</strong></h2>
<p>This application&#39;s src directory contains the following files</p>
<ol type="1" start="1">
<li>smoother.cpp : Defines a simple data structure that stores the smoothing operators weights and the routines for allocating memory, deallocating memory, and executing the smoothing operation.</li>
<li>main.cpp : Defines the main program that sets up the 2-D field to be smoothed and managed file IO.</li>
<li>makefile : A simple makefile is to build the application binary <code>smoother</code>.</li>
<li>viz.py : A python script for creating plots of the smoother output</li>
</ol>
<h2 is-upgraded><strong>Install and Verify the Application</strong></h2>
<p>To get started...</p>
<ol type="1" start="1">
<li>Clone the repository</li>
</ol>
<pre><code>$ git clone https://github.com/os-hackathon/amd-rocm-codelabs_example-codes</code></pre>
<ol type="1" start="2">
<li>Build the smoother application. Keep in mind, the compiler is set to gcc by default in the provided makefile.</li>
</ol>
<pre><code>$ cd c++/smoother/src
$ make</code></pre>
<ol type="1" start="3">
<li>Test run the example. The application takes two arguments. The first argument is the number of grid cells, and the second argument is the number of times the smoothing operator is applied.</li>
</ol>
<p>$ ./smoother 1000 100</p>
<h2 is-upgraded><strong>Visualize the output (Optional)</strong></h2>
<p>You can visualize the output with the provided <code>viz.py</code> python script. We recommend using virtual environments to install the script&#39;s dependencies</p>
<ol type="1" start="1">
<li>Start a virtual environment</li>
</ol>
<pre><code>$ python3 -m venv env
$ source env/bin/activate</code></pre>
<ol type="1" start="2">
<li>Install the required packages</li>
</ol>
<pre><code>(env)$ pip3 install -r requirements.txt</code></pre>
<ol type="1" start="3">
<li>Execute viz.py</li>
</ol>
<pre><code>(env)$ python3 ./viz.py</code></pre>
<p>This script saves a figure to <code>function.eps</code>. This figure shows the initial 2-D function before smoothing on the top and the smoothed field on the bottom. An example of the visualized output from the <code>smoother</code> example program is shown in the image below for a grid with 100x100 cells. The initial field is shown on the top, and the smoothed field is shown on the bottom after 100 iterations. Increasing the number of iterations (the second argument) will enhance the amount of smoothing and will further blur the image.<img style="width: 624.00px" src="img/36a5cdd828d91bbd.png"></p>
<h2 is-upgraded><strong>Profile the Application</strong></h2>
<p>Before starting any GPU porting exercise, it is important to profile your application to find hotspots where your application spends most of its time. Further, it is helpful to keep track of the runtime of the routines in your application so that you can later assess whether or not the GPU porting has resulted in improved performance. Ideally, your GPU-Accelerated application should outperform CPU-Only versions of your application when fully subscribed to available CPUs on a compute node.</p>
<aside class="special"><p><strong>Tip:</strong> To obtain a fair comparison between CPU-Only and GPU-Accelerated versions of your application,  you will want to compare the run-time between fully-subscribed CPU-only routines and the GPU-ported routines. </p>
<p>If your application is not parallelized on the CPU, you can estimate the idealized runtime on the CPU by dividing the serial runtime by the number of cores available on your target hardware.</p>
</aside>
<p>There are a number of open-source tools available for profiling C/C++ and Fortran applications, including <a href="https://www.cs.uoregon.edu/research/tau/home.php" target="_blank">Tau</a>, <a href="https://www.vi-hps.org/projects/score-p" target="_blank">Score-P</a>, <a href="https://vampir.eu/" target="_blank">Vampir</a>, and <a href="https://www.scalasca.org/" target="_blank">Scalasca</a>. In this tutorial, we are going to generate a profile and call graph using gprof. </p>
<h3 is-upgraded><strong>Create the profile</strong></h3>
<ol type="1" start="1">
<li>Add -pg flag to the CFLAGS variable in the provided Makefile.</li>
</ol>
<pre><code>CFLAGS=-O0 -g -pg</code></pre>
<ol type="1" start="2">
<li>Remove files from your previous build.</li>
</ol>
<pre><code>$ make clean</code></pre>
<ol type="1" start="3">
<li>Make the smoother application</li>
</ol>
<pre><code>$ make</code></pre>
<ol type="1" start="4">
<li>Run the application. When the -pg flag is passed to the gcc compiler, executions of the application will create a file called gmon.out</li>
</ol>
<pre><code>$ ./smoother 1000 10</code></pre>
<ol type="1" start="5">
<li>Create the profile</li>
</ol>
<pre><code>$ gprof ./smoother gmon.out &gt; profile.txt</code></pre>
<aside class="special"><p><strong>Tip:</strong> If you are working with compilers other than GNU compilers, you can use <a href="https://developer.mantidproject.org/ProfilingWithValgrind.html" target="_blank">Valgrind&#39;s Callgrind</a> command line tool to create call-graphs and to conduct hotspot analysis.</p>
</aside>
<h3 is-upgraded><strong>Interpret the profile and callgraph</strong></h3>
<p><code>gprof</code> provides a flat profile and a summary of your application&#39;s call structure indicating dependencies within your source code as a call graph. A <strong><em>call tree</em></strong> depicts the relationships between routines in your source code. Combining timing information with a call graph will help you plan the order in which you port routines to the GPU.</p>
<p>The first section of the gprof output is the flat-profile. An example flat-profile for the <code>smoother</code> application is given below. The flat-profile provides a list of routines in your application, ordered by the percent time your program spends within those routines from greatest to least. Beneath the flat-profile, gprof provides documentation of each of the columns for your convenience.</p>
<pre><code>  %   cumulative   self              self     total           
 time   seconds   seconds    calls  ms/call  ms/call  name    
 95.24      1.16     1.16       10   116.19   116.19  smoothField
  2.46      1.19     0.03       10     3.00     3.00  resetF
  2.46      1.22     0.03                             main
  0.00      1.22     0.00        1     0.00     0.00  smootherFree
  0.00      1.22     0.00        1     0.00     0.00  smootherInit</code></pre>
<p>Let&#39;s now take a look at at the call tree. This call tree has five entries, one for each routine in our program. The right-most field for each entry indicates the routines that called each routine and that are called by each routine. </p>
<p>For <code>smoother</code>, the first entry shows that main calls <code>smoothField</code>, <code>resetF</code>, <code>smootherInit</code>, and <code>smootherFree</code>. Further, the called column indicates that smoothField and resetF routines are shown to be called 10 times (in this case) by main. The self and children columns indicate that main spends 0.03s executing instructions in main and 1.19s in calling other routines. Further, of those 1.19s, 1.16s are spent in <code>smoothField</code> and 0.03 are spent in <code>resetF</code>. </p>
<pre><code>index % time    self  children    called     name
                                                 &lt;spontaneous&gt;
[1]    100.0    0.03    1.19                 main [1]
                1.16    0.00      10/10          smoothField [2]
                0.03    0.00      10/10          resetF [3]
                0.00    0.00       1/1           smootherInit [5]
                0.00    0.00       1/1           smootherFree [4]
-----------------------------------------------
                1.16    0.00      10/10          main [1]
[2]     95.1    1.16    0.00      10         smoothField [2]
-----------------------------------------------
                0.03    0.00      10/10          main [1]
[3]      2.5    0.03    0.00      10         resetF [3]
-----------------------------------------------
                0.00    0.00       1/1           main [1]
[4]      0.0    0.00    0.00       1         smootherFree [4]
-----------------------------------------------
                0.00    0.00       1/1           main [1]
[5]      0.0    0.00    0.00       1         smootherInit [5]
-----------------------------------------------</code></pre>
<aside class="special"><p><strong>Tip:</strong> You can use the open-source <a href="https://github.com/jrfonseca/gprof2dot" target="_blank">gprof2dot</a> to create visualizations of gprof output to help interpret the profile and call-graph for more complex applications.</p>
</aside>
<h3 is-upgraded><strong>Next steps</strong></h3>
<p>Now that we have a profile and an understanding of the call structure of the application, we can now plan our port to GPUs. First, we will focus on porting the smoothField routine and the necessary data to the GPU, since smoothField takes up the majority of the run time. </p>
<p>When we port this routine, we will introduce data allocation on the GPU and data copies between CPU and GPU. This data movement may potentially increase the overall application runtime, even if the smoothField routine performs better. In this event, we will then work on minimizing data movements between CPU and GPU. </p>
<aside class="special"><p class="image-container"><img style="width: 231.82px" src="img/ee2462ec9f6dcf9b.png"></p>
<p><strong>Tip:</strong> As a general strategy, it is recommended that you approach GPU porting in small incremental steps. Each step should consist of (1) profiling, (2) planning, (3) implementing planned changes &amp; verifying the application output, and (4) committing the changes to your repository.</p>
</aside>


      </google-codelab-step>
    
      <google-codelab-step label="Moving Data to the GPU with HIP" duration="20">
        <aside class="special"><p><strong>Tip: </strong>Before getting started in this section, make sure that <code>hipcc</code> is in your path by running <code>hipcc --version</code>. Additionally, if you are on an Nvidia platform, make sure that <code>nvcc</code> is in your path by running <code>nvcc --version</code>.</p>
</aside>
<p>In the <code>smoother</code> application, we have seen that the <code>smoothField</code> routine, called by <code>main</code>, takes up the most time. Looking at the function call in <code>main.cpp</code> and the <code>smoothField</code> routine in <code>smoother.cpp</code>, we see that this routine takes in a <code>smoother</code> object, a real array pointer <code>f</code>, and integers <code>nx</code> and <code>ny</code> that are passed by value.</p>
<pre><code> 81   for( int iter=0; iter&lt;nIter; iter++){
 82     // Run the smoother
 83     smoothField( &amp;smoothOperator, f, smoothF, nx, ny );
 84     // Reassign smoothF to f
 85     resetF( f, smoothF, nx, ny );
 86   } </code></pre>
<p>In order to offload <code>smoothField</code> to the GPU, we will need to copy <code>smoothOperator</code> class data and the <code>f</code> array to the GPU. After calling <code>smoothF</code>, we will eventually want to copy <code>smoothF</code> back to the CPU before calling <code>resetF</code>.</p>
<h2 is-upgraded><strong>Copy smoothOperator class data to the GPU</strong></h2>
<p>The <code>smoothField</code> routine uses the <code>smoothOperator -> weights</code> array when applying the operator. Because of this, you will need to create and allocate a device copy of the weights array. After filling in the weights values on the CPU, you can copy the values over to the device array.</p>
<ol type="1" start="1">
<li>Include the hip runtime header at the top of <code>smoother.c</code> so that you can make HIP API calls.</li>
</ol>
<pre><code>#include &lt;math.h&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &#34;precision.h&#34;
#include &#34;smoother.h&#34;
#include &lt;hip/hip_runtime.h&gt;</code></pre>
<ol type="1" start="2">
<li>Declare a real pointer attribute in the smoother class in <code>smoother.h</code> for a device copy of the smoother weights.</li>
</ol>
<pre><code>typedef struct smoother{
  int dim;
  real *weights;
  void *weights_dev;
}smoother;</code></pre>
<ol type="1" start="3">
<li>Insert a call to <a href="https://rocm-developer-tools.github.io/HIP/group__Memory.html#ga4c6fcfe80010069d2792780d00dcead2" target="_blank">hipMalloc</a> in <code>smootherInit</code> to allocate <code>weights_dev</code> on the GPU and insert a call to <a href="https://rocm-developer-tools.github.io/HIP/group__Memory.html#gac1a055d288302edd641c6d7416858e1e" target="_blank">hipMemcpy</a> in <code>smootherInit</code> to copy <code>weights</code> to <code>weights_dev</code>.</li>
</ol>
<pre><code>// Allocate space for the device copy of the smoothing weights
  hipMalloc(&amp;smoothOperator-&gt;weights_dev,N*N*sizeof(real));
// Copy weights from the host to the device
  hipMemcpy(smoothOperator-&gt;weights_dev,
            smoothOperator-&gt;weights,
            N*N*sizeof(real),
            hipMemcpyHostToDevice);</code></pre>
<ol type="1" start="4">
<li>Insert a call to <a href="https://rocm-developer-tools.github.io/HIP/group__Memory.html#ga740d08da65cae1441ba32f8fedb863d1" target="_blank">hipFree</a> in <code>smootherFree</code> to deallocate GPU memory held by <code>weights_dev</code>.</li>
</ol>
<pre><code>  hipFree(smoothOperator-&gt;weights_dev);</code></pre>
<ol type="1" start="5">
<li>Change the compiler in the <code>Makefile</code> to <code>hipcc</code> and set the necessary compiler flags. <br>(AMD Platforms only) Set the necessary compiler flags. You can determine your GPU target type by running <a href="https://github.com/RadeonOpenCompute/rocminfo" target="_blank"><code>rocminfo</code></a>. Once you have determined the target type, you can add <code>--amdgpu-target=TARGET</code>. <a href="https://github.com/RadeonOpenCompute/hcc/wiki#compiling-for-different-gpu-architectures" target="_blank">The ROCm Wiki</a> provides more information on compiling for different AMD GPU targets.</li>
</ol>
<pre><code>CC=hipcc
CFLAGS=-O0 -g
LFLAGS=-lm</code></pre>
<aside class="warning"><p><strong>Warning: </strong>You may also need to add <code>-D__HIP_PLATFORM_HCC__</code> (for AMD platforms) or <code>-D__HIP_PLATFORM_NVCC__</code> to the <code>CFLAGS</code> variable in the Makefile for ROCm version 3.5.</p>
</aside>
<aside class="special"><p><strong>Tip: </strong>To build code with hipcc on Nvidia platforms, you will need to set the environment variable <code>HIP_PLATFORM</code> to <code>nvcc</code>.</p>
<p><code>export HIP_PLATFORM=nvcc</code></p>
</aside>
<p>Once you have completed the code and Makefile modifications, you can now compile <code>smoother</code> and verify that data allocated and copied to the GPU.</p>
<ol type="1" start="1">
<li>Copy existing CPU data to a <code>reference/</code> subdirectory for later comparison. Whenever we make a change to the code, we will compare output with this reference data.</li>
</ol>
<pre><code>$ mkdir ./reference
$ mv function.txt smooth-function.txt ./reference/</code></pre>
<ol type="1" start="2">
<li>Remove the <code>*.o</code> files and the <code>smoother</code> binary to ensure a clean build and make a new <code>smoother</code> binary</li>
</ol>
<pre><code>$ make clean &amp;&amp; make smoother</code></pre>
<ol type="1" start="3">
<li>Run <code>smoother</code> with the same input parameters as you did in the previous section and verify the output is unchanged. We use the diff command line utility to compare the output files and the reference files. If there are no differences, diff will produce no output.</li>
</ol>
<pre><code>$ ./smoother 1000 10
$ diff function.txt reference/function.txt
$ diff smooth-function.txt reference.txt</code></pre>
<ol type="1" start="4">
<li>Verify that data is allocated on the GPU and that data is copied from the CPU to GPU by using a profiler.<br><br>To profile, use <code>rocprof</code> with the <code>--hip-trace on</code> and <code>--stats</code> flags. Running <code>rocprof</code> will create a file called <code>results.json</code> that contains the data for a trace profile. Additionally, <code>results.stats.csv</code> and <code>results.hip-stats.csv</code> will contain hotspot analysis for HIP kernels and HIP API calls, respectively.<br></li>
</ol>
<pre><code>$ rocprof --hip-trace --stats ./smoother 1000 10
&#34;Name&#34;,&#34;Calls&#34;,&#34;TotalDurationNs&#34;,&#34;AverageNs&#34;,&#34;Percentage&#34;
hipMemcpy,1,12503928,12503928,95.8586582624
hipMalloc,3,387867,129289,2.9734984242
hipFree,3,152335,50778,1.16784331343</code></pre>
<aside class="special"><p><strong>Tip:</strong> You can use <a href="https://www.chromium.org/developers/how-tos/trace-event-profiling-tool" target="_blank">Google Chrome Tracing</a> to visualize the results.json trace profile. Simply open the Google Chrome web browser and navigate to chrome://tracing and upload results.json. </p>
<p class="image-container"><img style="width: 610.00px" src="img/3682f6a6d97594e4.png"></p>
</aside>
<ol type="1" start="5">
<li>Commit your changes to your local git repository.</li>
</ol>
<pre><code>$ git add smoother.h smoother.cpp makefile &amp;&amp; git commit</code></pre>
<h2 is-upgraded><strong>Copy f and smoothF to the GPU</strong></h2>
<ol type="1" start="1">
<li>Include the hip runtime header at the top of <code>main.cpp</code> so that you can make HIP API calls.</li>
</ol>
<pre><code>#include &lt;math.h&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &#34;precision.h&#34;
#include &#34;smoother.h&#34;
#include &lt;hip/hip_runtime.h&gt;</code></pre>
<ol type="1" start="2">
<li>Declare real pointers for f and smoothF in main.cpp</li>
</ol>
<pre><code>int main( int argc, char *argv[] )  {
  smoother smoothOperator;
  int nx, ny, nElements;
  int nIter;
  real dx;
  real *f, *smoothF;
  real *f_dev, *smoothF_dev;</code></pre>
<ol type="1" start="3">
<li>Insert calls to hipMalloc for f_dev and smoothF_dev</li>
</ol>
<pre><code>  // Create the smoother
  smootherInit(&amp;smoothOperator);

  // Allocate space for the function we want to smooth
  f  = (real*)malloc( nElements*sizeof(real) );
  smoothF = (real*)malloc( nElements*sizeof(real) );

  hipMalloc(&amp;f_dev, nElements*sizeof(real));
  hipMalloc(&amp;smoothF_dev, nElements*sizeof(real));</code></pre>
<ol type="1" start="4">
<li>Insert hipMemcpy call to update f_dev (host to device) prior to calling smoothField and to update smoothF_dev (device to host) after calling smoothField.</li>
</ol>
<pre><code>    hipMemcpy(f_dev, f, nElements*sizeof(real), hipMemcpyHostToDevice);

    smoothField( &amp;smoothOperator, f, smoothF, nx, ny );

    hipMemcpy(smoothF, smoothF_dev, nElements*sizeof(real), hipMemcpyDeviceToHost);</code></pre>
<ol type="1" start="5">
<li>Verify that the application still builds. </li>
</ol>
<pre><code>$ make</code></pre>
<ol type="1" start="6">
<li>(Optional) You can verify that data is being copied between the CPU and GPU by using a profiler<br><br>For AMD platforms, use <code>rocprof</code> with the <code>--hip-trace</code> flag. Running rocprof will create a file called <code>profile.json</code>. The contents of <code>results.hip_stats.json</code> will show calls to <code>hipMalloc</code>, <code>hipMemcpy</code>, and <code>hipFree</code>.<br></li>
</ol>
<pre><code>&#34;Name&#34;,&#34;Calls&#34;,&#34;TotalDurationNs&#34;,&#34;AverageNs&#34;,&#34;Percentage&#34;
hipMemcpy,22,43602127,1981914,98.9534503691
hipMalloc,3,287919,95973,0.653421757999
hipFree,3,173225,57741,0.393127872872</code></pre>
<p><br>For Nvidia platforms, use <code>nvprof</code>. At this stage, you should see three calls to <code>cudaMalloc</code>, three calls to <code>cudaFree</code>, ten calls to <code>cudaMemcpy (Device to Host)</code>, and 11 calls to <code>cudaMemcpy (Host to Device)</code>.</p>
<pre><code>$ nvprof ./smoother 1000 10
==23287== NVPROF is profiling process 23287, command: ./smoother 1000 10
==23287== Profiling application: ./smoother 1000 10
==23287== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   52.12%  4.6421ms        10  464.21us  427.18us  589.69us  [CUDA memcpy DtoH]
                   47.88%  4.2636ms        11  387.60us  1.4720us  511.96us  [CUDA memcpy HtoD]
      API calls:   93.92%  194.18ms         3  64.727ms  135.75us  193.88ms  cudaMalloc
                    5.20%  10.757ms        21  512.25us  22.255us  778.80us  cudaMemcpy
                    0.39%  809.35us        97  8.3430us     537ns  396.19us  cuDeviceGetAttribute
                    0.25%  526.66us         1  526.66us  526.66us  526.66us  cuDeviceTotalMem
                    0.16%  339.77us         3  113.26us  2.7010us  176.22us  cudaFree
                    0.06%  115.21us         1  115.21us  115.21us  115.21us  cuDeviceGetName
                    0.00%  4.4240us         3  1.4740us     700ns  2.9380us  cuDeviceGetCount
                    0.00%  4.3810us         1  4.3810us  4.3810us  4.3810us  cuDeviceGetPCIBusId
                    0.00%  2.5650us         2  1.2820us     662ns  1.9030us  cuDeviceGet
                    0.00%  1.0790us         1  1.0790us  1.0790us  1.0790us  cuDeviceGetUuid</code></pre>
<aside class="warning"><p><strong>Caution: </strong>Copying smoothF_dev to <code>smoothF</code> will produce an incorrect result until the <code>smoothField</code> routine is offloaded to the GPU, which we will complete in the next section. This happens because <code>smoothF_dev</code> currently contains uninitialized data that we use to update <code>smoothF</code>.</p>
</aside>
<h2 is-upgraded><strong>Next steps</strong></h2>
<p>At this point, you now have the necessary data declared on the GPU. Additionally, you used hipMemcpy to make the input to smoothField available on the GPU. In the next step, you will create a HIP kernel that will run the smoothField algorithm on the GPU and replace the call to smoothField with a call to launch this kernel.</p>


      </google-codelab-step>
    
      <google-codelab-step label="HIP Kernel Launch Basics" duration="10">
        <p>Routines that are executed on the GPU are typically scheduled to run by issuing instructions from the host (CPU). In HIP, a GPU kernel is launched through a call to hipLaunchKernelGGL. When scheduling GPU kernel execution,  you specify </p>
<ol type="1" start="1">
<li>The kernel name,</li>
<li>The number of threads and the thread grouping,</li>
<li>The amount of Local Data Share (LDS) memory (Shared Memory) per block, and </li>
<li>The stream ID.</li>
</ol>
<h2 is-upgraded><strong>Threads, Blocks, and LDS Memory</strong></h2>
<p>When a kernel is launched, all requested threads on the GPU execute the kernel instructions concurrently. How the threads are scheduled to execute the instructions depends on the thread grouping. Threads are grouped into blocks in the HIP and CUDA programming models. Threads within a block are able to share LDS memory and L1 Cache on GPU Compute Units (Streaming Multiprocessor in Nvidia/CUDA terminology).</p>
<p>Within a HIP kernel, you are able to use HIP intrinsics to determine the unique ID of a thread. This allows you to codify memory access patterns for each thread within a kernel and expose SIMD parallelism. These intrinsics are summarized the table below.</p>
<table>
<tr><td colspan="1" rowspan="1"><p><strong>Intrinsic</strong></p>
</td><td colspan="1" rowspan="1"><p><strong>Description</strong></p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>hipBlockIDx_[x,y,z]</p>
</td><td colspan="1" rowspan="1"><p>The block ID in the [x,y,z] grid directions.</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>hipBlockDimx_[x,y,z]</p>
</td><td colspan="1" rowspan="1"><p>The number of threads within each block in the [x,y,z] grid directions.</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>hipThreadIDx_[x,y,z]</p>
</td><td colspan="1" rowspan="1"><p>The local thread ID within a block in the [x,y,z] block directions.</p>
</td></tr>
</table>
<aside class="special"><p><strong>Note:</strong> Block dimensions (threads-per-block) and Grid dimensions (number-of-blocks) are expressed as <a href="https://github.com/ROCm-Developer-Tools/HIP/blob/master/docs/markdown/hip_kernel_language.md#dim3" target="_blank">dim3</a> variables, giving 3 dimensions (x,y,z). </p>
</aside>
<h3 is-upgraded><strong>Example</strong></h3>
<p>To see how this works, consider this simple example kernel that takes in a device array and returns the same array multiplied by two.</p>
<pre><code>__global__ void myKernel(int N, double *d_a) {
  int i = hipThreadIdx_x + hipBlockIdx_x*hipBlockDim_x;
  if (i&lt;N) {
    d_a[i] *= 2.0;
  }
}</code></pre>
<p>In this example, when the kernel is launched, each thread will calculate i based on its thread ID within a block, its block ID, and the number of threads-per-block (hipBlockDimx_x). A conditional is added to ensure that threads only access in-bounds addresses of d_a. Provided this conditional is met for a thread, that thread will double the i-th element of d_a, concurrently with other threads.</p>
<h3 is-upgraded><strong>Best practice</strong></h3>
<p>On AMD GPUs, threads in a block are executed in 64-wide chunks called &#34;wavefronts&#34;. Because of this, it is good practice to make block sizes a multiple of 64 on AMD GPUs.</p>
<p>On Nvidia GPUs, threads in a block are executed in 32-wide chunks called &#34;warps&#34; and it is good practice to make the block size a multiple of 32.</p>
<aside class="warning"><p><strong>Caution: </strong>The number of threads per block, number of blocks, and the amount of local dynamic memory per block are limited; these limits can be found using <a href="https://github.com/RadeonOpenCompute/rocminfo" target="_blank">rocminfo</a> on AMD platforms and the <a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#running-binaries" target="_blank">deviceQuery CUDA-Toolkit example</a> on Nvidia platforms. </p>
</aside>
<h2 is-upgraded><strong>Streams</strong></h2>
<p>In this codelab, we will always set the stream ID to 0. Using multiple streams is useful when you want to run multiple GPU kernels concurrently. This strategy is useful when you are having difficulty keeping GPUs fully subscribed and you have code that has instruction-level parallelism.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Offload the Smoothing Kernel to the GPU" duration="20">
        <p>In this section, you will offload the smoothField routine to the GPU.</p>
<h2 is-upgraded><strong>Planning the GPU port</strong></h2>
<p>Let&#39;s look at the smoothField routine from smoother.c</p>
<pre><code>void smoothField( struct smoother *smoothOperator, real *f, real *smoothF, int nX, int nY )
{
  int iel, ism;
  int N = (real)smoothOperator-&gt;dim;
  int buf = (real)(smoothOperator-&gt;dim-1)/2.0;
  real smLocal;

  for( int j=buf; j &lt; nY-buf; j++ ){
    for( int i=buf; i &lt; nX-buf; i++ ){
      smLocal = 0.0;
      for( int jj=-buf; jj &lt;= buf; jj++ ){
        for( int ii=-buf; ii &lt;= buf; ii++ ){
          iel = (i+ii)+(j+jj)*nX;
          ism = (ii+buf) + (jj+buf)*N;
          smLocal += f[iel]*smoothOperator-&gt;weights[ism];
        }
      }
      iel = i+j*nX;
      smoothF[iel] = smLocal;
    }
  }
}</code></pre>
<p>The outer loops, over i and j, are tightly nested loops over a 2-D grid. The size of these loops are nY-2*buf and nX-2*buf. The values of nX and nY are determined by the user through the first command line argument (we have been using 1000), and buf is 2 (smoothOperator-&gt;dim=5; see smootherInit). Within the i and j loops, we carry out a reduction operation for smLocal and then assign the value to each element of smoothF.</p>
<p>In the smoothField algorithm, the order in which we execute the i and j loops does not matter. Further, the size of each loop is O(1000) for the example we&#39;re working with. A good strategy for offloading this routine to the GPU is to have each GPU thread execute the instructions within the i and j loops. Ideally, then we want each thread to execute something the following</p>
<pre><code>    real smLocal = 0.0;
    for( int jj=-buf; jj &lt;= buf; jj++ ){
      for( int ii=-buf; ii &lt;= buf; ii++ ){
        iel = (i+ii)+(j+jj)*nX;
        ism = (ii+buf) + (jj+buf)*N;
        smLocal += f[iel]*smoothOperator-&gt;weights[ism];
      }
    }
    iel = i+j*nX;
    smoothF[iel] = smLocal;</code></pre>
<p>Notice now the i and j loops are gone. Within the HIP kernel, we can calculate i and j from hipThreadIdx_[x,y], hipBlockIdx_[x,y], and hipBlockDim_[x,y], assuming that we will launch the kernel with 2-D Grid and Block dimensions. You can use something like the following to calculate i and j.</p>
<pre><code>  size_t i = hipThreadIdx_x + hipBlockIdx_x*hipBlockDim_x;
  size_t j = hipThreadIdx_y + hipBlockIdx_y*hipBlockDim_y;</code></pre>
<aside class="special"><p><strong>Note:</strong> hipThreadIdx_[x,y] and hipBlockIdx_[x,y] are 0-based indices.</p>
</aside>
<p>Within the main program, you will be able to launch the GPU kernel, but you will need to calculate the Grid and Block Dimensions. For now, let&#39;s assume that the number of threads-per-block in the i and j loop dimensions (x and y directions) is fixed at 16. With the number of threads-per-block (in each direction) chosen, you can calculate the grid dimensions, by requiring the x and y grid dimensions to be greater than or equal to the i and j loop sizes, respectively.</p>
<pre><code>  int buf = (real)(smoothOperator-&gt;dim-1)/2.0;
  int threadsPerBlockX = 16;
  int threadsPerBlockY = 16;
  int gridDimX = (nX-2*buf)/threadsPerBlockX + 1;
  int gridDimY = (nY-2*buf)/threadsPerBlockY + 1;</code></pre>
<aside class="special"><p><strong>Note:</strong> In general, the number of threads-per-block will impact the application runtime and you will want to use a suitable profiler to help optimize performance. </p>
<p>On AMD platforms, threads in a block are executed in 64-wide chunks called &#34;wavefronts&#34; and it is good practice is to make the block size a multiple of 64.</p>
<p>On Nvidia platforms, threads in a block are executed in 32-wide chunks called &#34;warps&#34; and it is good practice is to make the block size a multiple of 32.</p>
</aside>
<h2 is-upgraded><strong>Implement the Changes</strong></h2>
<ol type="1" start="1">
<li>Add a new routine, <code>smoothField_gpu</code>, to <code>smoother.cpp</code>. This routine needs to be of type __global__ so that it can be launched on the GPU (device) from the CPU (host) using hipLaunchKernelGGL.</li>
</ol>
<pre><code>__global__ void smoothField_gpu( real *weights_dev, real *f_dev, real *smoothF_dev, int nX, int nY, int N )
{
  int buf = (real)(N-1)/2.0;
  size_t i = hipThreadIdx_x + hipBlockIdx_x*hipBlockDim_x + buf;
  size_t j = hipThreadIdx_y + hipBlockIdx_y*hipBlockDim_y + buf;
  int iel, ism;
  
  if( i &gt;= buf &amp;&amp; i &lt; nX-buf &amp;&amp; j &gt;= buf &amp;&amp; j&lt; nY-buf){
    real smLocal = 0.0;
    for( int jj=-buf; jj &lt;= buf; jj++ ){
      for( int ii=-buf; ii &lt;= buf; ii++ ){
        iel = (i+ii)+(j+jj)*nX;
        ism = (ii+buf) + (jj+buf)*N;
        smLocal += f_dev[iel]*weights_dev[ism];
      }
    }
    iel = i+j*nX;
    smoothF_dev[iel] = smLocal;
  }
}</code></pre>
<aside class="special"><p><strong>Note:</strong> We&#39;ve added <code>buf</code> to <code>i</code> and <code>j</code> since the original loops began at <code>i=buf</code> and <code>j=buf</code>. Also, notice that we&#39;ve added conditionals to safely ensure that threads do not step outside of the memory bounds of <code>f_dev</code>, <code>weights_dev</code>, and <code>smoothF_dev</code>.</p>
</aside>
<ol type="1" start="2">
<li>Calculate the Block and Grid dimensions in <code>main.cpp</code>. You can place this block of code just before the iteration loop</li>
</ol>
<pre><code>  int buf = (real)(smoothOperator.dim-1)/2.0;
  int threadsPerBlockX = 16;
  int threadsPerBlockY = 16;
  int gridDimX = (nX-2*buf)/threadsPerBlockX + 1;
  int gridDimY = (nY-2*buf)/threadsPerBlockY + 1;</code></pre>
<ol type="1" start="3">
<li>Replace the smoothField call in main.cpp with a kernel launch call for smoothField_gpu</li>
</ol>
<pre><code>    // Copy f from host to device : f is input to `smoothField`
    hipMemcpy(f_dev, f, nElements*sizeof(real), hipMemcpyHostToDevice);

    // Run the smoother
    hipLaunchKernelGGL((smoothField_gpu), dim3(gridDimX,gridDimY,1), dim3(threadsPerBlockX,threadsPerBlockY,1), 0, 0,
                        smoothOperator.weights_dev, f_dev, smoothF_dev, nx, ny, smoothOperator.dim );

    // Copy smoothF_dev from device to host
    hipMemcpy(smoothF, smoothF_dev, nElements*sizeof(real), hipMemcpyDeviceToHost);</code></pre>
<ol type="1" start="4">
<li>Add <code>smoothField_gpu</code> declaration to <code>smoother.h</code></li>
</ol>
<pre><code>__global__ void smoothField_gpu( real* weights, real *f, real *smoothF, int nX, int nY, int N);</code></pre>
<ol type="1" start="5">
<li>Rebuild the application. For Nvidia Platforms, you will need to set <code>CFLAGS=-O0 -g -fmad=false</code> before recompiling.</li>
</ol>
<aside class="warning"><p><strong>Caution: </strong>In order to obtain bit-for-bit correctness on Nvidia Platforms, you must add the <code>-fmad=false</code> flag to the <code>CFLAGS</code> variable in the Makefile. This flag disables fused multiply-add operations. Fused Multiply-Add (FMA) operations on the GPU result in improved performance but can cause a difference in the computed solution between <code>smoothField</code> and <code>smoothField_gpu</code>.</p>
</aside>
<pre><code>$ make clean &amp;&amp; make</code></pre>
<ol type="1" start="6">
<li>Verify that the solution output agrees with your reference output</li>
</ol>
<pre><code>$ ./smoother 1000 10
$ diff function.txt reference/function.txt
$ diff smooth-function.txt reference/smooth-function.txt</code></pre>
<ol type="1" start="7">
<li>You can verify that the HIP Kernel is running on the GPU by using a profiler.</li>
</ol>
<ol type="1" start="1">
<li>For AMD platforms, use <code>rocprof</code> with the <code>--hip-trace</code> and <code>--stats</code> flags. Running rocprof will create a file called <code>results.stats.csv</code> that contains a summary of the kernels that are run on the GPU. </li>
</ol>
<pre><code>&#34;Name&#34;,&#34;Calls&#34;,&#34;TotalDurationNs&#34;,&#34;AverageNs&#34;,&#34;Percentage&#34;
&#34;smoothField_gpu(float*, float*, float*, int, int, int)&#34;,10,278624770,27862477,99.9749094615</code></pre>
<p>The results.hip_stats.csv file shows the HIP API calls that are executed. Notice that, for this example (10 iterations), hipMemcpy is called 22 times.</p>
<pre><code>&#34;Name&#34;,&#34;Calls&#34;,&#34;TotalDurationNs&#34;,&#34;AverageNs&#34;,&#34;Percentage&#34;
hipMemcpy,22,334403308,15200150,97.2313227317
hipModuleLaunchKernel,20,8892061,444603,2.58546142385
hipMalloc,3,329911,109970,0.0959251363438
hipFree,3,300215,100071,0.0872907081227</code></pre>
<ol type="1" start="2">
<li>For Nvidia platforms, use <code>nvprof</code>. </li>
</ol>
<pre><code>$ nvprof ./smoother 1000 10
==9394== NVPROF is profiling process 9394, command: ./smoother 1000 10
==9394== Profiling application: ./smoother 1000 10
==9394== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   53.96%  10.307ms        10  1.0307ms  1.0293ms  1.0337ms  smoothField_gpu(float*, float*, float*, int, int, int)
                   23.97%  4.5788ms        12  381.57us  1.4720us  522.30us  [CUDA memcpy HtoD]
                   22.07%  4.2150ms        10  421.50us  407.71us  463.90us  [CUDA memcpy DtoH]
      API calls:   89.23%  189.13ms         3  63.045ms  105.94us  188.86ms  cudaMalloc
                   10.09%  21.383ms        22  971.95us  17.322us  1.7658ms  cudaMemcpy
                    0.25%  533.80us        97  5.5030us     172ns  379.31us  cuDeviceGetAttribute
                    0.24%  512.54us         3  170.85us  163.89us  174.59us  cudaFree
                    0.09%  186.85us         1  186.85us  186.85us  186.85us  cuDeviceTotalMem
                    0.08%  162.32us        10  16.232us  7.4320us  50.629us  cudaLaunchKernel
                    0.02%  38.694us         1  38.694us  38.694us  38.694us  cuDeviceGetName
                    0.00%  3.8060us         1  3.8060us  3.8060us  3.8060us  cuDeviceGetPCIBusId
                    0.00%  1.3010us         3     433ns     208ns     825ns  cuDeviceGetCount
                    0.00%     766ns         2     383ns     186ns     580ns  cuDeviceGet
                    0.00%     300ns         1     300ns     300ns     300ns  cuDeviceGetUuid</code></pre>
<aside class="special"><p><strong>Tip:</strong> Once you have verified your results, make sure that you commit your changes to your local git repository.</p>
</aside>
<h2 is-upgraded><strong>Next steps</strong></h2>
<p>Congratulations! So far, you&#39;ve learned how to allocate and manage memory on the GPU and how to launch a GPU kernel. Right now, we have the code in a state where, every iteration, data is copied to the GPU before calling smoothField_gpu and from the GPU after calling smoothField_gpu. This situation happens quite often when porting to GPUs for the first time.</p>
<p>The next step in this codelab is to offload the resetF routine to the GPU, even though it does not take up a lot of time. We want to offload it to the GPU so that we can move the hipMemcpy calls outside of the iteration loop in main and reduce the number of times data is transmitted across the PCI bus.</p>


      </google-codelab-step>
    
      <google-codelab-step label="(Optional) Offload the resetF Kernel to the GPU" duration="10">
        <p>In this section, we are going to offload the <code>resetF</code> routine in <code>smoother.cpp</code> to the GPU so that we can migrate <code>hipMemcpy</code> calls outside of the iteration loop in <code>main.cpp</code>. By this point, you have worked through the mechanics for porting a routine to the GPU. Additionally, for this application, we already have all of the necessary data on the GPU that the <code>resetF</code> routine depends on.</p>
<ol type="1" start="1">
<li>Add <code>resetF_gpu</code> definition to <code>smoother.h</code></li>
</ol>
<pre><code>__global__ void resetF_gpu( real *f, real *smoothF, int nx, int ny, int buf );</code></pre>
<ol type="1" start="2">
<li>Add <code>resetF_gpu</code> to <code>smoother.cpp</code></li>
</ol>
<pre><code>__global__ void resetF_gpu( real *f, real *smoothF, int nx, int ny, int buf )
{
  size_t i = hipThreadIdx_x + hipBlockIdx_x*hipBlockDim_x + buf;
  size_t j = hipThreadIdx_y + hipBlockIdx_y*hipBlockDim_y + buf;
  int iel = i + nx*j;
  if( i &gt;= buf &amp;&amp; i &lt; nX-buf &amp;&amp; j &gt;= buf &amp;&amp; j&lt; nY-buf){
    f[iel] = smoothF[iel];
  }
}</code></pre>
<ol type="1" start="3">
<li>Replace <code>resetF</code> with <code>resetF_gpu</code> in <code>main.cpp</code>, move the <code>hipMemcpy</code> host-to-device calls before the iteration loop, and move the <code>hipMemcpy</code> device-to-host call after the iteration loop.</li>
</ol>
<pre><code>  hipMemcpy(f_dev, f, nElements*sizeof(real), hipMemcpyHostToDevice);

  for( int iter=0; iter&lt;nIter; iter++){

    // Run the smoother
    hipLaunchKernelGGL((smoothField_gpu), dim3(gridDimX,gridDimY,1), dim3(threadsPerBlockX,threadsPerBlockY,1), 0, 0,
                        smoothOperator.weights_dev, f_dev, smoothF_dev, nx, ny, smoothOperator.dim );

    // Reassign smoothF to f
    hipLaunchKernelGGL((resetF_gpu), dim3(gridDimX,gridDimY,1), dim3(threadsPerBlockX,threadsPerBlockY,1), 0, 0, f_dev, smoothF_dev, nx, ny, buf );
  }
  // Copy smoothF_dev from device to host
  hipMemcpy(smoothF, smoothF_dev, nElements*sizeof(real), hipMemcpyDeviceToHost);</code></pre>
<ol type="1" start="4">
<li>Add <code>resetF_gpu</code> declaration to <code>smoother.h</code></li>
</ol>
<pre><code>__global__ void smoothField_gpu( real* weights, real *f, real *smoothF, int nX, int nY, int N);</code></pre>
<ol type="1" start="5">
<li>Recompile the smoother application</li>
</ol>
<pre><code>$ make</code></pre>
<ol type="1" start="6">
<li>Verify that the solution output agrees with your reference output</li>
</ol>
<pre><code>$ ./smoother 1000 10
$ diff function.txt reference/function.txt
$ diff smooth-function.txt reference/smooth-function.txt</code></pre>
<ol type="1" start="7">
<li>You can verify that the the number of hipMemcpy calls is now reduced.</li>
</ol>
<ol type="1" start="1">
<li>For AMD platforms, use <code>rocprof</code> with the <code>--hip-trace</code> flag. Running rocprof with the --hip-trace flag will create a file called <code>results.hip_stats.csv</code> that contains a summary of the HIP API calls that are executed. Notice that the number of hipMemcpy calls has been reduced to 4, from 22 calls.</li>
</ol>
<pre><code>&#34;Name&#34;,&#34;Calls&#34;,&#34;TotalDurationNs&#34;,&#34;AverageNs&#34;,&#34;Percentage&#34;
hipMemcpy,4,301495556,75373889,96.9712661155
hipModuleLaunchKernel,20,8972482,448624,2.88585659862
hipMalloc,3,301029,100343,0.0968212057742
hipFree,3,143194,47731,0.0460560801107</code></pre>
<ol type="1" start="2">
<li>For Nvidia platforms, use <code>nvprof</code>. </li>
</ol>
<pre><code>$ nvprof ./smoother 1000 10
==5017== NVPROF is profiling process 5017, command: ./smoother 1000 10
==5017== Profiling application: ./smoother 1000 10
==5017== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   83.03%  9.3196ms        10  931.96us  931.14us  934.88us  smoothField_gpu(float*, float*, float*, int, int, int)
                    7.37%  826.91us         3  275.64us  1.4400us  415.81us  [CUDA memcpy HtoD]
                    5.91%  662.82us        10  66.281us  65.536us  67.616us  resetF_gpu(float*, float*, int, int, int)
                    3.70%  415.27us         1  415.27us  415.27us  415.27us  [CUDA memcpy DtoH]
      API calls:   93.01%  167.17ms         3  55.725ms  106.73us  166.94ms  cudaMalloc
                    6.40%  11.511ms         4  2.8777ms  11.740us  10.515ms  cudaMemcpy
                    0.27%  493.11us         3  164.37us  152.91us  180.26us  cudaFree
                    0.11%  202.57us        97  2.0880us     204ns  83.892us  cuDeviceGetAttribute
                    0.11%  197.07us         1  197.07us  197.07us  197.07us  cuDeviceTotalMem
                    0.06%  111.06us        20  5.5530us  3.8360us  29.368us  cudaLaunchKernel
                    0.02%  37.002us         1  37.002us  37.002us  37.002us  cuDeviceGetName
                    0.00%  4.3920us         1  4.3920us  4.3920us  4.3920us  cuDeviceGetPCIBusId
                    0.00%  2.0330us         3     677ns     237ns  1.4970us  cuDeviceGetCount
                    0.00%     951ns         2     475ns     243ns     708ns  cuDeviceGet
                    0.00%     393ns         1     393ns     393ns     393ns  cuDeviceGetUuid
</code></pre>
<aside class="special"><p><strong>Tip:</strong> Once you have verified your results, make sure that you commit your changes to your local git repository.</p>
</aside>


      </google-codelab-step>
    
      <google-codelab-step label="Congratulations" duration="0">
        <p>In this codelab, you learned how to port serial CPU-only routines in C to GPUs using HIP. To do this, you created device copies of CPU arrays and learned how to copy data from the CPU to the GPU and vice versa. You also learned how to write HIP kernels and launch them from the host. </p>
<p>In the process of doing this, you practiced a strategy for porting to GPUs that included the following steps to make incremental changes to your own source code :</p>
<ol type="1" start="1">
<li>Profile - Find out the hotspots in your code and understand the dependencies with other routines</li>
<li>Plan - Determine what routine you want to port, what data needs to be present on the GPU, and what data needs to be copied back to the CPU after execution</li>
<li>Implement &amp; Verify - Create the necessary device data, insert the appropriate hipMemcpy calls, write an equivalent GPU kernel, and use hipLaunchKernelGGL to launch the GPU kernel. Run your application&#39;s tests and verify the results are correct. Check with a profiler that the new routine and the necessary hipMemCpy calls are being executed.</li>
<li>Commit - Once you have verified correctness and the expected behavior, commit your changes and start the process over again.</li>
</ol>
<h2 is-upgraded><strong>Provide Feedback</strong></h2>
<p>If you have any questions, comments, or feedback that can help improve this codelab, you can <a href="https://github.com/os-hackathon/amd-rocm-codelabs/issues/new" target="_blank"><strong>open an issue on the os-hackathon/amd-rocm-codelabs Github repository</strong></a>.</p>
<h2 is-upgraded>Further reading</h2>
<ul>
<li><a href="https://rocmdocs.amd.com/en/latest/Programming_Guides/HIP-GUIDE.html" target="_blank">HIP Programming Guide</a></li>
<li><a href="https://rocmdocs.amd.com/en/latest/Programming_Guides/Programming-Guides.html#hip-documentation" target="_blank">HIP Documentation</a></li>
<li><a href="https://rocmdocs.amd.com/en/latest/" target="_blank">About AMD ROCm</a></li>
<li><a href="https://rocm-developer-tools.github.io/HIP/" target="_blank">HIP API Documentation</a></li>
<li><a href="https://docs.nvidia.com/cuda/floating-point/index.html#fused-multiply-add-fma" target="_blank">Fused Multiply-Add on Nvidia hardware</a></li>
</ul>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/codelab-elements/native-shim.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/prettify.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>

</body>
</html>
